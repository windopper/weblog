---
title: "Transformer"
description: ""
tags: ["ML"]
date: "2023-01-20"
thumbnail: "/markdown-memo/legacy/images/5aa184ab-ce63-4cab-831c-21337f54f2d8.png"
---

<!-- Table of Contents -->


# Transformer

íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)ëŠ” 2017ë…„ êµ¬ê¸€ì´ ë°œí‘œí•œ ë…¼ë¬¸ì¸ **Attention is all you need**ì—ì„œ ë‚˜ì˜¨ ëª¨ë¸ë¡œ ê¸°ì¡´ì˜ seq2seqì˜ êµ¬ì¡°ì¸ ì¸ì½”ë”-ë””ì½”ë”ë¥¼ ë”°ë¥´ë©´ì„œë„, ë…¼ë¬¸ì˜ ì´ë¦„ì²˜ëŸ¼ ì–´í…ì…˜(Attention)ë§Œìœ¼ë¡œ êµ¬í˜„í•œ ëª¨ë¸ì´ë‹¤. ì´ ëª¨ë¸ì€ RNNì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ì˜€ìŒì—ë„ ë²ˆì—­ì„±ëŠ¥ì—ì„œë„ RNNë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.


![íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ëª¨ë¸ ì•„í‚¤í…ì²˜](/markdown-memo/legacy/images/5aa184ab-ce63-4cab-831c-21337f54f2d8.png)

# ê¸°ì¡´ seq2seq ëª¨ë¸ì˜ í•œê³„?

ì¸ì½”ë”ê°€ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ì••ì¶•í•˜ëŠ” ê³¼ì •ì—ì„œ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ì •ë³´ê°€ ì¼ë¶€ ì†ì‹¤ëœë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.


# Positional Encoding

íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” RNNê³¼ ë‹¬ë¦¬ ë‹¨ì–´ ì…ë ¥ì„ ìˆœì°¨ì ìœ¼ë¡œ ë°›ëŠ” ë°©ì‹ì´ ì•„ë‹ˆë¯€ë¡œ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ì•Œë¦´ í•„ìš”ê°€ ìˆë‹¤. ë”°ë¼ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” í•´ë‹¹ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•˜ì—¬ ê° ë‹¨ì–´ì˜ ì„ë² ë”© ë²¡í„°ì— ìœ„ì¹˜ ì •ë³´ë“¤ì„ ë”í•˜ì—¬ ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©, ì´ë¥¼ positional encoding ì´ë¼ê³  í•œë‹¤.

<!-- Unknown block type: column_list -->

# Positional Encoding êµ¬í˜„í•˜ê¸°

# Attention

> Attentionì€ ì¸ì§€ì‹¬ë¦¬í•™ì—ì„œì˜ ì£¼ì˜ë¥¼ ëª¨ë°©í•˜ì—¬ ê³ ì•ˆëœ ê¸°ìˆ . Attentionì€ ì…ë ¥ ë°ì´í„° ì¤‘ ì¼ë¶€ì˜ íš¨ê³¼ë¥¼ ì¦ê°•ì‹œí‚¤ë©°, ë‹¤ë¥¸ ì¼ë¶€ë¥¼ ê°ì†Œì‹œí‚¨ë‹¤. ì´ëŠ” ë„¤íŠ¸ì›Œí¬ê°€ ë°ì´í„° ì¤‘ ë¹„ì¤‘ì´ ì ì§€ë§Œ ì¤‘ìš”í•œ ë°ì´í„°ì— ë” ì§‘ì¤‘í•˜ê²Œ í•˜ê¸° ìœ„í•´ì„œì´ë‹¤. ë°ì´í„° ì¤‘ ì–´ëŠ ë¶€ë¶„ì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ ë‹¤ë¥¸ ë¶€ë¶„ì„ í•™ìŠµí•˜ëŠ” ê²ƒë³´ë‹¤ ë” ì¤‘ìš”í•œì§€ëŠ” ë¬¸ë§¥ì— ë”°ë¼ ê²°ì •ë˜ë©°, ì´ëŠ” [ê²½ì‚¬í•˜ê°•ë²•](/7a15a8ee2694466db2f635d257381fc2)ìœ¼ë¡œ í•™ìŠµëœë‹¤.

Attentionì€ ê¸°ê³„ ë²ˆì—­ taskë¥¼ ë” ì˜ í’€ê¸° ìœ„í•´ ê³ ì•ˆë˜ì—ˆìœ¼ë©°, RNNì— ê¸°ë°˜í•œ seq2seq ëª¨ë¸ì˜ ê³ ì§ˆì ì¸ ë¬¸ì œì¸ **ì •ë³´ ì••ì¶•ìœ¼ë¡œ ì¸í•œ ì •ë³´ ì†ì‹¤** ê·¸ë¦¬ê³  **ê¸°ìš¸ê¸° ì†Œì‹¤**ì— ì˜í•œ ë²ˆì—­ ì •í™•ë„ ê°ì†Œë¥¼ ë³´ì •í•´ì£¼ê¸° ìœ„í•˜ì—¬ ë“±ì¥í•œ ê¸°ë²•ì´ë‹¤.


ì–´í…ì…˜ í•¨ìˆ˜ëŠ” **ì¿¼ë¦¬(Query)ì™€ Key-Value ì§‘í•© ìŒ**ì„ ì¿¼ë¦¬(Query), í‚¤(Keys), ê°’(Values) ê·¸ë¦¬ê³  ê²°ê³¼(Output)ê°€ ëª¨ë‘ ë²¡í„°ì¸ **Output**ì— ë§¤í•‘í•˜ëŠ” ê²ƒìœ¼ë¡œ ì„¤ëª… í•  ìˆ˜ ìˆë‹¤. 

ê²°ê³¼(Output)ì€ valuesì˜ ê°€ì¤‘ í•©ìœ¼ë¡œ ê³„ì‚°ë˜ë©°, ì—¬ê¸°ì„œ ê° valueì— í• ë‹¹ëœ ê°€ì¤‘ê°’ì€ Queryì™€ í•´ë‹¹ Keyì˜ compatibility functionìœ¼ë¡œ ê³„ì‚°ëœ ê°’ì´ë‹¤.


Q : ì˜í–¥ì„ ë°›ëŠ” ë²¡í„°

K : ì˜í–¥ì„ ì£¼ëŠ” ë²¡í„°

V : ì£¼ëŠ” ì˜í–¥ì˜ ê°€ì¤‘ì¹˜ ë²¡í„°

![](/markdown-memo/legacy/images/48d9e97a-b506-430e-b127-538e71b3f9ea.png)

## Different Use for multi-head attention

Transformer ëª¨ë¸ì€ multi-head attentionì„ ì„¸ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì‚¬ìš©í•œë‹¤.

### encoder-decoder attention

### self-attention in encoder

### self-attention in decoder


---


# Encoder


# self-attention in encoder

seq2seqì—ì„œ ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ëŠ” ê²½ìš° Q, K, Vì˜ ì •ì˜ë¥¼ ë‹¤ì‹œ ìƒê°í•´ ë³¸ë‹¤ë©´,

> ğŸ’¡ Q = Query : t ì‹œì ì˜ ë””ì½”ë” ì…€ì—ì„œì˜ ì€ë‹‰ ìƒíƒœ
K = Keys : ëª¨ë“  ì‹œì ì˜ ì¸ì½”ë” ì…€ì—ì„œì˜ ì€ë‹‰ ìƒíƒœë“¤
V = Values : ëª¨ë“  ì‹œì ì˜ ì¸ì½”ë” ì…€ì˜ ì€ë‹‰ ìƒíƒœë“¤


ê·¸ëŸ¬ë‚˜ self-attention ì—ì„œëŠ” Q, K, V ê°€ ëª¨ë‘ ë™ì¼í•˜ë¯€ë¡œ,

> ğŸ’¡ Q : ì…ë ¥ ë¬¸ì¥ì˜ ëª¨ë“  ë‹¨ì–´ ë²¡í„°ë“¤
K : ì…ë ¥ ë¬¸ì¥ì˜ ëª¨ë“  ë‹¨ì–´ ë²¡í„°ë“¤
V : ì…ë ¥ ë¬¸ì¥ì˜ ëª¨ë“  ë‹¨ì–´ ë²¡í„°ë“¤

## Q, K, V ë²¡í„° êµ¬í•˜ê¸°

self-attentionì€ ì…ë ¥ ë¬¸ì¥ì˜ ë‹¨ì–´ ë²¡í„°ë“¤ì„ ê°€ì§€ê³  ê³„ì‚°ì„ ìˆ˜í–‰í•œë‹¤ê³  í–ˆìœ¼ë‚˜, self-attentionì€ encoderì˜ ì´ˆê¸° ì…ë ¥ì¸ $d_{model}$ì˜ ì°¨ì›ì„ ê°€ì§€ëŠ” ë‹¨ì–´ ë²¡í„°ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì…€í”„ ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ **ê° ë‹¨ì–´ ë²¡í„°ë“¤ë¡œë¶€í„° ****$Q, K, V$****ë²¡í„°ë“¤ì„ ì–»ëŠ” ì‘ì—…ì„ ê±°ì³¤ë‹¤.**

ë…¼ë¬¸ì—ì„œëŠ” $d_{model}=512$ì˜ ì°¨ì›ì„ ê°€ì¡Œë˜ ê° ë‹¨ì–´ ë²¡í„°ë“¤ì„ $64$ì˜ ì°¨ì›ì„ ê°€ì§€ëŠ” $Q, K, V $ë¡œ ë³€í™˜í•˜ì˜€ë‹¤.

$64$ë¼ëŠ” ê°’ì€ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ë˜ ë‹¤ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì¸ $h$ì— ì˜í•˜ì—¬ ê²°ì •ë˜ëŠ”ë°, íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” $d_{model}$ì„ $h$ë¡œ ë‚˜ëˆˆ ê°’ì„ ê° $Q, K, V$ì˜ ì°¨ì›ì„ ê²°ì •í•˜ì˜€ë‹¤.

ë…¼ë¬¸ì—ì„œëŠ” $h = 8$ë¡œ ì •í•˜ì˜€ë‹¤.


![](/markdown-memo/legacy/images/6432cee6-8517-43be-9b3c-f3ac2a109858.png)

ì…ë ¥ ë¬¸ì¥ì˜ ë‹¨ì–´ ë²¡í„°ë“¤ ë³´ë‹¤ ë” ì‘ì€ ë²¡í„°ë“¤ì€ ê°€ì¤‘ì¹˜ í–‰ë ¬ $W^Q, W^K, W^V$ë¥¼ ê³±í•˜ì—¬ ì™„ì„±ëœë‹¤.

ê° ê°€ì¤‘ì¹˜ í–‰ë ¬ì€ $d_{model}\times{({d_{model}/{h}})}$ì˜ í¬ê¸°ë¥¼ ê°€ì§€ê³ , í•´ë‹¹ í–‰ë ¬ë“¤ì€ í›ˆë ¨ ê³¼ì •ì—ì„œ í•™ìŠµëœë‹¤.

## Scaled dot-product Attention

<!-- Unknown block type: column_list -->

attention í•¨ìˆ˜ì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ê²ƒì€ **dot-product attention**ê³¼** additive attention**ì´ë‹¤. dot-product attentionì€ scaling factorì¸ $\sqrt{d_k}$ì„ ì œì™¸í•˜ë©´ ìƒë‹¨ì˜ ì•Œê³ ë¦¬ì¦˜ê³¼ ë™ì¼í•˜ë©°, additive attentionì€ ë‹¨ì¼ hidden layerì—ì„œ FFN(feed-forward network)ì„ ì‚¬ìš©í•˜ì—¬ compatibility functionì„ ê³„ì‚°í•œë‹¤.

ë‘ ë°©ë²•ì€ ì´ë¡ ì ìœ¼ë¡œ ë³µì¡ë„ê°€ ë¹„ìŠ·í•˜ì§€ë§Œ, **dot-product attention**ì€ matrixë¥¼ í†µí•˜ì—¬ ìµœì í™”ëœ ì—°ì‚°ì„ êµ¬í˜„ í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— í›¨ì”¬ ë¹ ë¥´ê³  ê³µê°„ì ìœ¼ë¡œ íš¨ìœ¨ì ì´ë‹¤.

$d_k$ì˜ ê°’ì´ ì‘ë‹¤ë©´ ë‘ ë§¤ì»¤ë‹ˆì¦˜ë“¤ì€ ë¹„ìŠ·í•œ ì†ë„ë¡œ ì‘ë™í•˜ì§€ë§Œ $d_k$ì˜ ê°’ì´ í´ ë•Œ $1\over{\sqrt{d_k}}$ë¡œ ìŠ¤ì¼€ì¼ë§ì„ í•˜ì§€ ì•Šìœ¼ë©´ **additive attention**ë³´ë‹¤ ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤. ìŠ¤ì¼€ì¼ë§ì„ í•˜ì§€ ì•Šìœ¼ë©´ dot product ê°’ì´ ë§¤ìš° ì»¤ì„œ softmax í•¨ìˆ˜ë¥¼ í†µê³¼í•˜ì˜€ì„ë•Œì˜ gradientê°€ ë§¤ìš° ì‘ì€ ì´ìœ ë¡œ í•™ìŠµì´ ì˜ ë˜ì§€ ì•Šì•„ ì´ëŸ¬í•œ ë¶€ì‘ìš©ì„ ë°©ì§€í•˜ê¸° ìœ„í•˜ì—¬ dot product ê°’ì„ $1\over\sqrt{d_k}$ë¡œ ìŠ¤ì¼€ì¼ë§ í•˜ì˜€ë‹¤.


$$
Attention(Q, K, V) = softmax({QK^T\over\sqrt{d_k}})V
$$

í•´ë‹¹ ìˆ˜ì‹ì„ NLP ëª¨ë¸ë¡œ ì‚¬ìš©í–ˆë‹¤ê³  ê°€ì •í•œ í›„ ë‹¤ì‹œ ë³´ì.

Nice to meet you ë¼ëŠ” ë¬¸ì¥ì—** attention** ì„ ì ìš©í•œë‹¤ë©´,

**Query**ë¥¼ í•œ ë‹¨ì–´( ex: Nice ) ê·¸ë¦¬ê³  **Key**ë¥¼ ëª¨ë“  ë‹¨ì–´ë“¤ì˜ **stacked matrix**ë¼ê³  í•œë‹¤.

$QK^T$ëŠ” **í•œ ë‹¨ì–´( ex: Nice )**ì™€ **ëª¨ë“  ë‹¨ì–´( ex: Nice, to , meet, you )**ë“¤ì˜ **dot product**ë¥¼ í•´ì£¼ì–´ ì–´ë– í•œ **relation vector**ë¥¼ ë§Œë“¤ì–´ ë‚¸ë‹¤. ì´ ê³¼ì •ì€ ëª¨ë“  ë‹¨ì–´ë“¤ì´ **Query**ë¡œ ì‚¬ìš©ë  ë•Œ ê¹Œì§€ ë°˜ë³µí•œë‹¤.

> ğŸ’¡ ì‹¤ì œë¡œëŠ”** Queryë¥¼ Keyì™€ ê°™ì´ stack**í•˜ì—¬ matrixê°„ dot productë¥¼ í†µí•˜ì—¬ relation matrixë¥¼ ë§Œë“ ë‹¤.

ê·¸ í›„, $1\over\sqrt{d_k}$ë¡œ ìŠ¤ì¼€ì¼ë§ í›„, $softmax$í•¨ìˆ˜ë¥¼ í†µí•˜ì—¬ **Query ë‹¨ì–´ê°€ ëª¨ë“  ë‹¨ì–´ ë“¤ê³¼ ì–´ëŠ ì •ë„ì˜ correlation (ìƒê´€ê´€ê³„)**ê°€ ìˆëŠ”ì§€ë¥¼ í™•ë¥  ë¶„í¬ë¡œ ë§Œë“  í›„, ì´ë¥¼ Value matrixì™€ dot productë¥¼ í•´ì£¼ì–´, ê¸°ì¡´ Vectorì— **Query**ì™€ **Key**ê°„ì˜ correlation** **ì •ë³´ë¥¼ ë”í•œ Vectorë¥¼ ë§Œë“ ë‹¤. 

ì—¬ê¸°ì„œ ì™„ì„±ëœ $Attention(Q, K, V)$ì˜ ê°’ì„ **Attention Value (ì–´í…ì…˜ ê°’)** ì´ë¼ê³  ë¶€ë¥¸ë‹¤.


í•´ë‹¹ í–‰ë ¬ ì—°ì‚°ì— ì‚¬ìš©ëœ í–‰ë ¬ì˜ í¬ê¸°ë¥¼ ëª¨ë‘ ì •ë¦¬í•´ë³´ì.

ì…ë ¥ ë¬¸ì¥ì˜ ê¸¸ì´ **seq_len **ë¼ê³  ê°€ì •í•œë‹¤.

ë¬¸ì¥ í–‰ë ¬ì˜ ê¸¸ì´ëŠ” $(seq\_len, d_{model})$ì´ë‹¤. ì—¬ê¸°ì— 3ê°œì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê³±í•˜ì—¬ $Q, K, V $í–‰ë ¬ì„ ë§Œë“¤ì–´ì•¼ í•œë‹¤.

$Q, K$ì˜ ì°¨ì›ì„ $d_k$ë¼ê³  í•˜ê³ , $V$ë²¡í„°ì˜ ì°¨ì›ì„ $d_v$ë¼ê³  í•˜ë©´. $Q, K$í–‰ë ¬ì˜ í¬ê¸°ëŠ” $(seq\_len, d_k)$ê°€ ë˜ê³  $V$í–‰ë ¬ì˜ í¬ê¸°ëŠ” $(seq\_len, d_v)$ê°€ ëœë‹¤.

ë”°ë¼ì„œ ë¬¸ì¥ í–‰ë ¬ê³¼ Q, K, Ví–‰ë ¬ì˜ í¬ê¸°ë¡œ ë¶€í„° ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ í¬ê¸° ì¶”ì •ì´ ê°€ëŠ¥í•´ ì§„ë‹¤.

$W^Q, W^K$ëŠ” $(d_{model}, d_k)$ì˜ í¬ê¸°ë¥¼ ê°€ì§€ë©°,  $W^V$ëŠ” $(d_{model}, d_v)$ì˜ í¬ê¸°ë¥¼ ê°€ì§„ë‹¤.

ë…¼ë¬¸ì—ì„œëŠ” $d_k$ì™€ $d_v$ì˜ ì°¨ì›ì€ $d_{model}/h$ì™€ ê°™ìœ¼ë¯€ë¡œ, ì¦‰, $d_{model}/h = d_k = d_v$ ì´ë‹¤.

ê²°ê³¼ì ìœ¼ë¡œ $softmax({QK^T\over\sqrt{d_k}})V$ ì‹ì„ ì ìš©í•˜ì—¬ ë‚˜ì˜¤ëŠ” attention ê°’ í–‰ë ¬ì€ $(seq\_len, d_v)$ ì´ë‹¤.


### Padding Mask

Scaled dot-product attention í•¨ìˆ˜ ë‚´ë¶€ë¥¼ ë³´ë©´ mask ê°’ì„ ì˜µì…˜ì„ ì„¤ì • í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì´ ìˆë‹¤.

Mask LayerëŠ” ì°¸ì¡°í•˜ê³  ì‹¶ì§€ ì•Šì€ correlationì„ maskingí•  ë•Œ ì‚¬ìš©í•˜ëŠ” ì¸µìœ¼ë¡œ ì‹¤ì§ˆì ì¸ ì˜ë¯¸ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•Šê±°ë‚˜ ì°¸ì¡°í•˜ê³ ì í•˜ëŠ” ë°ì´í„°ë“¤ì˜ ì ‘ê·¼ ì—¬ë¶€ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ <PAD>ê°€ í¬í•¨ëœ ì…ë ¥ ë¬¸ì¥ì˜ self-attentionì˜ ì˜ˆì œë¥¼ ë³´ì.

![](/markdown-memo/legacy/images/b8f09ea8-b4df-4c7f-a817-97b417efb73a.png)


ë‹¨ì–´ <PAD>ëŠ” ì‹¤ì§ˆì ì¸ ì˜ë¯¸ë¥¼ ê°€ì§„ ë‹¨ì–´ê°€ ì•„ë‹ˆë¯€ë¡œ íŠ¸ëœìŠ¤í¬ë¨¸ì—ê²Œ ì´ì— ëŒ€í•œ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ì§€ ì•Šë„ë¡ í•˜ê¸° ìœ„í•˜ì—¬ Maskingì„ í•´ì¤€ë‹¤.

Maskingì´ë€ Attention ê³¼ì •ì—ì„œ ì œì™¸í•˜ê¸° ìœ„í•œ ê°’ì„ ê°€ë¦°ë‹¤ëŠ” ì˜ë¯¸ë‹¤.

Maskingì„ í•˜ëŠ” ë°©ë²•ì€ Attention Score Matrixì˜ Masking ìœ„ì¹˜ì— ë§¤ìš° ì‘ì€ ìŒìˆ˜ê°’ì„ ë„£ì–´ ì£¼ëŠ” ê²ƒì´ë‹¤ ( ex : -1000000000ì™€ ê°™ì€ ìŒìˆ˜ë¬´í•œëŒ€ì— ê°€ê¹Œìš´ ìˆ˜ ).

Maskingì„ ë§ˆì¹œ Attention Score MatrixëŠ” ë‹¤ìŒ ì—°ì‚°ìœ¼ë¡œ softmax í•¨ìˆ˜ë¥¼ ì§€ë‚˜ê²Œ ë˜ëŠ”ë°, í˜„ì¬ ë§ˆìŠ¤í‚¹ ìœ„ì¹˜ì— ë§¤ìš° ì‘ì€ ìŒìˆ˜ ê°’ì´ ë“¤ì–´ê°€ ìˆìœ¼ë¯€ë¡œ softmax í•¨ìˆ˜ë¥¼ ì§€ë‚œ í›„ì˜ ìœ„ì¹˜ëŠ” 0ì´ ë˜ì–´ ë‹¨ì–´ê°„ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ëŠ” ì¼ì— <PAD> í† í°ì´ ë°˜ì˜ë˜ì§€ ì•Šê²Œ í•œë‹¤.

![](/markdown-memo/legacy/images/7c34fda4-7900-46b0-8df5-8b9cc473e0a4.png)

### Padding Mask êµ¬í˜„í•˜ê¸°

## scaled dot-product attention êµ¬í˜„í•˜ê¸°

## Multi-Head Attention

${d_{model}}$ì°¨ì›ì˜ K, V, Qì„ single attention functionì— ë„£ëŠ” ê²ƒ ë³´ë‹¤, ê°ê° $d_k, d_k, d_v$ì˜ ì°¨ì›ì— ëŒ€í•´ í•™ìŠµëœ ì„œë¡œ ë‹¤ë¥¸ linear projectionsë¥¼ ì‚¬ìš©í•˜ì—¬ Q, K, Vë¥¼ $h$íšŒ linearly projectí•˜ëŠ” ê²ƒì´ í›¨ì”¬ ìœ ìµí•˜ë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œê²Œë˜ì—ˆë‹¤.

ì´ëŸ¬í•œ Q, K, Vì˜ projected versionsì„ ë³‘ë ¬ì ìœ¼ë¡œ attention function (ì—¬ê¸°ì„œëŠ” Scaled Dot-Product Attention) ì—ì„œ ê³„ì‚°í•˜ì—¬ $d_v$ì°¨ì›ì˜ ê²°ê³¼ ê°’ì„ ë„ì¶œí•˜ì˜€ë‹¤.

ë„ì¶œëœ ê°’ì„ concatenated í•œ í›„, ë‹¤ì‹œ projecting í•˜ì—¬ ìµœì¢… ê°’ì„ ì–»ì—ˆë‹¤.

Multi-head attentionì„ í†µí•˜ì—¬ ëª¨ë¸ì€ ì„œë¡œ ë‹¤ë¥¸ í‘œí˜„ì˜ ì •ë³´ì— ê³µí†µìœ¼ë¡œ ì£¼ì˜ë¥¼ ê¸°ìš¸ì¼ ìˆ˜ ìˆë‹¤.

**ì •ë¦¬í•˜ìë©´, ë³‘ë ¬ì ì¸ ì–´í…ì…˜ì„ í†µí•˜ì—¬ ë‹¤ì–‘í•œ ì–´í…ì…˜ í—¤ë“œì˜ ì‹œê°ìœ¼ë¡œ ì—°ê´€ë„ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ íŒë‹¨í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.**

<!-- Unknown block type: column_list -->

$d_{model}$ì„ 512ì´ë¼ê³  í•˜ê³  $h$ê°€ 8ì´ë¼ê³  í•˜ë©´, Query, Key, Valueì˜ dimensionì€ $d_k = d_v = d_{model}/h = 64$ë¡œ $h$ì˜ ê°œìˆ˜ë¡œ ë‚˜ëˆ„ì–´ì£¼ì–´ dimensionì´ ì¤„ì–´ë“¤ê¸° ë•Œë¬¸ì— single-head attentionì„ í–ˆì„ ë•Œì™€ ë¹„êµí•´ì„œ ë¹„ìŠ·í•œ computational costë¥¼ ê°€ì§„ë‹¤.


$Concat(head_1,...,head_h)$ì˜ í¬ê¸°ëŠ” $(seq\_len, h\times{d_v})$

$W^O$ì˜ í¬ê¸°ëŠ” $(hd_v, d_{model})$ ì´ë¯€ë¡œ

$MultiHead(Q, K, V)$ë¥¼ ëª¨ë‘ ìˆ˜í–‰í•˜ì˜€ë‹¤ë©´ í¬ê¸°ëŠ” $(seq\_len, d_{model})$ê°€ ëœë‹¤.


## Multi-head Attention êµ¬í˜„í•˜ê¸°

# Position-Wise FFNN

í¬ì§€ì…˜ ì™€ì´ì¦ˆ FFNN (Fully-connected FFNN) ì€ ì¸ì½”ë”ì™€ ë””ì½”ë”ì—ì„œ ê³µí†µì ìœ¼ë¡œ ê°€ì§€ê³  ìˆëŠ” ì„œë¸Œì¸µì´ë‹¤.

$FFNN(x) = MAX(0, xW_1 + b_1)W_2+b _2$

$x$ëŠ” ì•ì„œ multi-head attention ì˜ ê²°ê³¼ë¡œ ë‚˜ì˜¨ $(num\_len, d_{model})$ì˜ í¬ê¸°ë¥¼ ê°€ì§€ëŠ” í–‰ë ¬ì´ë‹¤.

ê°€ì¤‘ì¹˜ í–‰ë ¬ $W_1$ì€ $(d_{model}, d_{ff})$ì˜ í¬ê¸°ë¥¼ ê°€ì§€ê³ , ê°€ì¤‘ì¹˜ í–‰ë ¬ $W_2$ì€ $(d_{ff}, d_{model})$ì˜ í¬ê¸°ë¥¼ ê°€ì§„ë‹¤.

$MAX(0, xW_1 + b_1)$ëŠ” $xW_1 + b_1$ë¥¼ í™œì„±í™” í•¨ìˆ˜ì¸ ReLUì— í†µê³¼í•œ ê²ƒì´ë‹¤.

í•´ë‹¹ ë…¼ë¬¸ì—ì„œ ì€ë‹‰ì¸µì˜ í¬ê¸°ëŠ” $d_{ff}$ëŠ” 2048ë¡œ ì •ì˜í•˜ì˜€ë‹¤.


# Position-Wise FFNN êµ¬í˜„í•˜ê¸°

```python
outputs = tf.keras.layers.Dense(units=dff, activation='relu')
outputs = tf.keras.layers.Dense(units=d_model)(outputs)
```


# Residual connection and Layer normalization

## ì”ì°¨ ì—°ê²° (Residual connection)

<!-- Unknown block type: link_to_page -->

ì„œë¸Œì¸µì´ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì´ë¯€ë¡œ ì”ì°¨ ì—°ê²° ì—°ì‚°ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$H(x) = x+Multi\_Head\_Attention(x)$

## ì¸µ ì •ê·œí™” (Layer normalization)

<!-- Unknown block type: link_to_page -->

ì¸µ ì •ê·œí™”ëŠ” **í…ì„œì˜ ë§ˆì§€ë§‰ ì°¨ì›**ì— ëŒ€í•´ì„œ í‰ê· ê³¼ ë¶„ì‚°ì„ êµ¬í•œ í›„ ê°’ì„ ì •ê·œí™”í•œë‹¤.

ì—¬ê¸°ì„œ í…ì„œì˜ ë§ˆì§€ë§‰ ì°¨ì›ì´ë¼ëŠ” ê²ƒì€ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œëŠ” $d_{model}$ì°¨ì›ì„ ì˜ë¯¸í•œë‹¤.

![](/markdown-memo/legacy/images/8dbf5f5d-9e00-449c-9dec-56bbaf8c678d.png)

ì¸µ ì •ê·œí™”ë¥¼ ìœ„í•˜ì—¬ í™”ì‚´í‘œ ë°©í–¥ìœ¼ë¡œ ê°ê° í‰ê·  $\mu$ê³¼ ë¶„ì‚° $\sigma^2$ì„ êµ¬í•œë‹¤. ê° í™”ì‚´í‘œ ë°©í–¥ì˜ ë²¡í„°ë¥¼ $x_i$ë¼ê³  ëª…ëª…í•œë‹¤.

ì¸µ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•œ í›„ì—ëŠ” ë²¡í„°$x_i$ëŠ” $ln_i$ë¼ëŠ” ë²¡í„°ë¡œ ì •ê·œí™”ê°€ ëœë‹¤.

$ln_i=LayerNorm(x_i)$

ì”ì°¨ ì—°ê²° í›„ ì •ê·œí™” ê³¼ì •ì„ ê±°ì¹œ ì—°ì‚°ì„ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ìë©´,

$LN = LayerNorm(x+Multi\_Head\_Attention(x))$


# Encoder êµ¬í˜„í•˜ê¸°

```python
def encoder_layer(dff, d_model, h, dropout, name="encoder_layer"):
  inputs = tf.keras.Input(shape=(None, d_model), name="inputs")

  """
    ì¸ì½”ë”ì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” ë¬¸ìì—ëŠ” <PAD> í† í° ì¦‰, íŒ¨ë”©ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ
    ì–´í…ì…˜ ì‹œ íŒ¨ë”© í† í°ì„ ì œì™¸í•˜ë„ë¡ íŒ¨ë”© ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©.
  """
  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')

  # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ( ì²«ë²ˆì§¸ ì–´í…ì…˜ / ì…€í”„ ì–´í…ì…˜ )
  attention = MultiHeadAttention(
      d_model, h, name='attention'
  )({
      'query': inputs, 'key': inputs, 'value': inputs,
      'mask': padding_mask
  })

  # ë“œë¡­ì•„ì›ƒ + ì”ì°¨ ì—°ê²°ê³¼ ì¸µ ì •ê·œí™”
  attention = tf.keras.layers.Dropout(rate=dropout)(attention)
  attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)

  # í¬ì§€ì…˜ ì™€ì´ì¦ˆ í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ ( ë‘ë²ˆì§¸ ì‹ ê²½ë§ )
  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)
  outputs = tf.keras.layers.Dense(units=d_model)(outputs)

  # ë“œë¡­ì•„ì›ƒ + ì”ì°¨ ì—°ê²°ê³¼ ì¸µ ì •ê·œí™”
  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)
  outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention+outputs)

  return tf.keras.Model(
      inputs=[inputs, padding_mask], outputs=outputs, name=name
  )
```

íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” h ê°œìˆ˜ ë§Œí¼ ì¸ì½”ë” ì¸µì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ë¥¼ ì—¬ëŸ¬ë²ˆ ìŒ“ëŠ” ì½”ë“œë¥¼ ë³„ë„ë¡œ êµ¬í˜„í•´ ì£¼ì–´ì•¼ í•œë‹¤.


# Encoder ìŒ“ê¸°

```python
def encoder(seq_len, num_layers, dff, d_model, h, dropout, name='encoder'):
  inputs = tf.keras.Input(shape=(None, ), name='inputs')

  # ì¸ì½”ë”ëŠ” íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©
  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')

  embeddings = tf.keras.layers.Embedding(seq_len, d_model)
  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))
  embeddings = PositionalEncoding(seq_len, d_model)(embeddings)
  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)

  # ì¸ì½”ë”ë¥¼ num_layerê°œ ìŒ“ê¸°
  for i in range(num_layers):
    outputs = encoder_layer(dff=dff, d_model=d_model, h=h, dropout=dropout,
                            name="encoder_layer_{}".format(i))([outputs, padding_mask])

  return tf.keras.Model(
      inputs=[inputs, padding_mask], outputs=outputs, name=name)   
  )
```

# Decoder

# ë””ì½”ë”ì˜ ì²«ë²ˆì§¸ ì„œë¸Œì¸µ : self-attention and look-ahead mask

![](/markdown-memo/legacy/images/966e87c0-f4a8-4fc5-a75f-c16d8887b2b3.png)

decoderë„ encoderì™€ ë™ì¼í•œë° ì„ë² ë”© ì¸µê³¼ í¬ì§€ì…”ë„ ì¸ì½”ë”©ì„ ê±°ì¹œ í›„ì˜ ë¬¸ì¥ í–‰ë ¬ì´ ì…ë ¥ëœë‹¤.

ë‹¤ë§Œ ì—¬ê¸°ì„œ ë¬¸ì œê°€ ìˆëŠ”ë°, seq2seq2ì˜ ë””ì½”ë”ì— ì‚¬ìš©ë˜ëŠ” RNN ê³„ì—´ì˜ ì‹ ê²½ë§ì€ ì…ë ¥ ë‹¨ì–´ë¥¼ ë§¤ ì‹œì ë§ˆë‹¤ ìˆœì°¨ì ìœ¼ë¡œ ì…ë ¥ë°›ìœ¼ë¯€ë¡œ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì— í˜„ì¬ ì‹œì ì„ í¬í•¨í•œ ì´ì „ ì‹œì ì— ì…ë ¥ëœ ë‹¨ì–´ë“¤ë§Œ ì°¸ê³ í•  ìˆ˜ ìˆë‹¤. ë°˜ë©´, íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ë¬¸ì¥ í–‰ë ¬ì„ í•œë²ˆì— ì…ë ¥ë°›ìœ¼ë¯€ë¡œ í˜„ì¬ ì‹œì ì˜ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê³ ì í•  ë•Œ, ì…ë ¥ ë¬¸ì¥ í–‰ë ¬ë¡œë¶€í„° ë¯¸ë˜ ì‹œì ì˜ ë‹¨ì–´ê¹Œì§€ë„ ì°¸ê³ í•  ìˆ˜ ìˆëŠ” í˜„ìƒì´ ë°œìƒí•œë‹¤.

ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ë””ì½”ë”ì—ì„œëŠ” í˜„ì¬ ì‹œì ì˜ ì˜ˆì¸¡ì—ì„œ í˜„ì¬ ì‹œì ë³´ë‹¤ ë¯¸ë˜ì— ìˆëŠ” ë‹¨ì–´ë“¤ì„ ì°¸ê³ í•˜ì§€ ëª»í•˜ë„ë¡ look-ahead maskë¥¼ ë„ì…í•˜ì˜€ë‹¤.


![](/markdown-memo/legacy/images/013ba882-88dc-4858-89f6-409ff2ca5791.png)

look-ahead maskëŠ” ë””ì½”ë”ì˜ ì²«ë²ˆì§¸ ì„œë¸Œì¸µì—ì„œ ì´ë£¨ì–´ì§„ë‹¤. ë””ì½”ë”ì˜ ì²« ë²ˆì§¸ ì„œë¸Œì¸µì¸ Multi-head self-attention ì¸µì€ ì¸ì½”ë”ì˜ ì²«ë²ˆì§¸ ì„œë¸Œì¸µì¸ ë©€í‹° í—¤ë“œ ì…€í”„ ì–´í…ì…˜ ì¸µê³¼ ë™ì¼í•œ ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤. ì˜¤ì§ ë‹¤ë¥¸ ì ì€ Attention Score Matrixì—ì„œ ë§ˆìŠ¤í‚¹ì„ ì ìš©í•œë‹¤ëŠ” ì ë§Œ ë‹¤ë¥´ë‹¤.

![](/markdown-memo/legacy/images/1ed9ff8b-7197-45a0-aef3-d2810840477e.png)

ì´ì œ ìê¸° ìì‹ ë³´ë‹¤ ë¯¸ë˜ì— ìˆëŠ” ë‹¨ì–´ë“¤ì€ ì°¸ê³ í•˜ì§€ ëª»í•˜ë„ë¡ ë§ˆìŠ¤í‚¹ ë˜ì—ˆë‹¤.

ë§ˆìŠ¤í‚¹ ëœ í›„ì˜ Attention Score Matrixì˜ ê° í–‰ì„ ë³´ë©´ ìê¸° ìì‹ ê³¼ ê·¸ ì´ì „ ë‹¨ì–´ë“¤ë§Œì„ ì°¸ê³ í•  ìˆ˜ ìˆìŒì„ ë³¼ ìˆ˜ ìˆë‹¤.

# ë””ì½”ë”ì˜ ë‘ë²ˆì§¸ ì„œë¸Œì¸µ : encoder-decoder attention

ë””ì½”ë”ì˜ ë‘ë²ˆì§¸ ì„œë¸Œì¸µì€ Multi-head attentionì„ ìˆ˜í–‰í•œë‹¤ëŠ” ì ì—ì„œëŠ” ì´ì „ì˜ ì–´í…ì…˜ë“¤ê³¼ëŠ” ê³µí†µì ì´ ìˆìœ¼ë‚˜ self-attentionì´ ì•„ë‹ˆë‹¤.

self-attentionì€ Query, Key, Valueê°€ ê°™ì€ ê²½ìš°ë¥¼ ë§í•˜ì§€ë§Œ encoder-decoder attentionì€ Queryê°€ decoderì¸ í–‰ë ¬ì¸ ë°˜ë©´, Keyì™€ ValueëŠ” ì¸ì½”ë” í–‰ë ¬ì´ê¸° ë•Œë¬¸ì´ë‹¤.

![](/markdown-memo/legacy/images/420c80b5-9993-4f7b-a8ae-afab24d72141.png)

# References

[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)

[https://wikidocs.net/22893](https://wikidocs.net/22893)

[https://ko.wikipedia.org/wiki/ì£¼ì˜_ê¸°ì œ](https://ko.wikipedia.org/wiki/%EC%A3%BC%EC%9D%98_%EA%B8%B0%EC%A0%9C)

[https://velog.io/@yuns_u/ë‹¨ìœ„ë²¡í„°-ê¸°ì €ë²¡í„°-span-rank-linear-projections](https://velog.io/@yuns_u/%EB%8B%A8%EC%9C%84%EB%B2%A1%ED%84%B0-%EA%B8%B0%EC%A0%80%EB%B2%A1%ED%84%B0-span-rank-linear-projections)

[https://aistudy9314.tistory.com/63](https://aistudy9314.tistory.com/63)

[https://wikidocs.net/31379](https://wikidocs.net/31379)

[https://silhyeonha-git.tistory.com/16](https://silhyeonha-git.tistory.com/16)

[https://skyjwoo.tistory.com/entry/positional-encodingì´ë€-ë¬´ì—‡ì¸ê°€](https://skyjwoo.tistory.com/entry/positional-encoding%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80)


<!-- Unknown block type: child_page -->

<!-- Unknown block type: child_page -->

<!-- Unknown block type: child_page -->

