---
title: "Generative Adversarial Nets"
description: ""
tags: ["ML","Paper Review"]
date: "2023-01-24"
thumbnail: "/markdown-memo/legacy/images/9b62590b-a529-4b6d-8963-b519c9606d8d.png"
---

ì‘ì„±ì¤‘ 

<!-- Table of Contents -->

# Abstract

# Introduction

# Related Work

# Adversarial nets

ì ëŒ€ì  ëª¨ë¸ í”„ë ˆì„ì›Œí¬ëŠ” ë‘ ëª¨ë¸ì´ MLP(Multi Layer Perceptron) ì¼ë•Œ ê°€ì¥ ì§ê´€ì ìœ¼ë¡œ ì ìš©ëœë‹¤.

ë°ì´í„° $x$ì— ëŒ€í•´ ìƒì„±ìì˜ distribution $p_g$ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” **ì…ë ¥ ë…¸ì´ì¦ˆ ë³€ìˆ˜ ****$P_z(z)$****ì— ëŒ€í•œ ì‚¬ì „ ì •ì˜**ë¥¼ í•œ í›„, **ë°ì´í„° ê³µê°„ì— ëŒ€í•œ ë§¤í•‘ì„ ****$G(z;\theta_g)$**ë¡œ í‘œí˜„í•œë‹¤. ì—¬ê¸°ì„œ $G$ëŠ” ë§¤ê°œë³€ìˆ˜ $\theta_g$ê°€ ìˆëŠ” MLPì— ì˜í•˜ì—¬ í‘œí˜„ë˜ëŠ” ë¯¸ë¶„ê°€ëŠ¥í•œ í•¨ìˆ˜ì´ë‹¤.

ë˜í•œ ìš°ë¦¬ëŠ” **ë‹¨ì¼ ìŠ¤ì¹¼ë¼ë¥¼ ì¶œë ¥í•˜ëŠ” ë‘ë²ˆì§¸ MLP ****$D(x;\theta_d)$****ë¥¼ ì •ì˜**í•œë‹¤.

$D(x)$ëŠ” $x$ê°€ $p_g$ê°€ ì•„ë‹Œ **ë°ì´í„°ì—ì„œ ë‚˜ì˜¨ í™•ë¥ **ì„ ë‚˜íƒ€ë‚¸ë‹¤.

ìš°ë¦¬ëŠ” **í›ˆë ¨ ì˜ˆì‹œë“¤ê³¼ ****$G$****ì—ì„œ ë§Œë“¤ì–´ì§„ ìƒ˜í”Œ**ë“¤ì— ëŒ€í•œ **ì˜¬ë°”ë¥¸ ë¼ë²¨ì„ ë§ì¶œ í™•ë¥ ì„ ìµœëŒ€í™”í•˜ëŠ” ****$D$**ë¥¼ í›ˆë ¨ì‹œí‚¨ë‹¤.

ìš°ë¦¬ëŠ” ë™ì‹œì— $log(1-D(G(z)))$ë¥¼ ìµœì†Œí™” í•˜ëŠ” $G$ë¥¼ í›ˆë ¨ì‹œí‚¨ë‹¤.


ë‹¬ë¦¬ë§í•˜ë©´, $D$ì™€ $G$ëŠ” ê°€ì¹˜ í•¨ìˆ˜ $V(G, D)$ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ 2ì¸ ë¯¸ë‹ˆë§¥ìŠ¤ ê²Œì„ì„ ì§„í–‰í•œë‹¤.

$$
\underset{G}{min}\underset{D}{max}V(D,G)=\mathbb{E}_{x\sim p_{data}(x)}[logD(x)]+\mathbb{E}_{z\sim p_z(z)}[log(1-D(G(z)))]
$$

> ğŸ’¡ *dataë¡œ ë¶€í„° ë½‘ì€ sample **$x$**ì— ëŒ€í•´ **$D(x)=1$**ì´ ë˜ê³ , **$G$**ì— ì„ì˜ì˜ ë…¸ì´ì¦ˆ ë³€ìˆ˜ë¡œë¶€í„° ë½‘ì€ input **$z$**ì„ ë„£ê³  ë§Œë“  sampleì— ëŒ€í•´ì„œëŠ” **$D(G(z))=0$**ì´ ë˜ë„ë¡ ë…¸ë ¥í•œë‹¤.

****ì¦‰, ******$D$******ëŠ” ì˜¬ë°”ë¥¸ ë¼ë²¨ì„ ë§ì¶œ í™•ë¥ ì„ ë§ì¶”ê¸° ìœ„í•´ ( ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ******$max$******í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ) ë…¸ë ¥í•˜ê³ , ë°˜ëŒ€ë¡œ ******$G$******ëŠ” ******$D$******ê°€ ì˜¬ë°”ë¥¸ ë¼ë²¨ì„ ë§ì¶œ í™•ë¥ ì„ ë‚®ì¶”ê¸° ìœ„í•´ ( ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ******$min$******í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ) ë…¸ë ¥í•œë‹¤.***


ë‹¤ìŒ ì„¹ì…˜ì—ì„œëŠ” $G$ì™€ $D$ì— ì¶©ë¶„í•œ ìš©ëŸ‰, ì¦‰ ë¹„ëª¨ìˆ˜ì  í•œê³„ê°€ ì£¼ì–´ì¡Œì„ ë•Œ í›ˆë ¨ ê¸°ì¤€ì´ ë°ì´í„° ìƒì„± ë¶„í¬ë¥¼ ë³µêµ¬í•  ìˆ˜ ìˆìŒì„ ë³¸ì§ˆì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ì ëŒ€ì  ë„¤íŠ¸ì›Œí¬ì˜ ì´ë¡ ì  ë¶„ì„ì„ ì œì‹œí•œë‹¤.

ì‹¤ì œë¡œ, ìš°ë¦¬ëŠ” ë°˜ë³µì ì´ê³  ìˆ˜ì¹˜ì ì¸ ì ‘ê·¼ì„ ì‚¬ìš©í•˜ì—¬ ê²Œì„ì„ êµ¬í˜„í•´ì•¼ í•œë‹¤.

í›ˆë ¨ ë£¨í”„ ê³¼ì •ì—ì„œ ì™„ë£Œê¹Œì§€ $D$ì— ëŒ€í•œ ìµœì í™”ë¥¼ í•˜ëŠ” ê²ƒì€ ê³„ì‚°ì ìœ¼ë¡œ ê¸ˆì§€ë˜ì–´ ìˆê³  ì œí•œëœ ë°ì´í„° ì…‹ì€ ì˜¤ë²„í”¼íŒ…ì„ ì•¼ê¸°í•  ê²ƒì´ë‹¤.

**ëŒ€ì‹ ì—, ìš°ë¦¬ëŠ” ****$D$****ì˜ ìµœì í™” ****$k$****ë²ˆê³¼ ****$G$****ì˜ ìµœì í™” í•œë²ˆì„ ë²ˆê°ˆì•„ í•  ê²ƒì´ë‹¤.**

**ì´ ê²°ê³¼ëŠ” ****$G$****ê°€ ëŠë¦¬ê²Œ ë³€í•˜ëŠ” í•œ, ****$D$****ê°€ optimal solutionì— ê°€ê¹Œì›Œì§€ë„ë¡ ìœ ì§€ ë˜ì–´ì§ˆ ê²ƒì´ë‹¤.**


![](/markdown-memo/legacy/images/9b62590b-a529-4b6d-8963-b519c9606d8d.png)

í•´ë‹¹ ìë£ŒëŠ”** ****discriminative distribution (D, íŒŒë€ ì ì„ )**, **íŒë³„ ëª¨ë¸ì˜ ë¶„í¬ë„ë¥¼ ê°±ì‹ **í•˜ì—¬ ì›ë˜ **ë°ì´í„°ë¡œë¶€í„° ë§Œë“¤ì–´ì§„ ****$x$****ì˜ ë¶„í¬ë„ ****$p_x$**** (ê²€ì€ìƒ‰ ì ì„ ) **ì™€ **generative distribution ****$p_g$**** (G, ì´ˆë¡ìƒ‰ ì ì„ )**, **ìƒì„± ëª¨ë¸ë¡œ ë¶€í„° ìƒì„±ëœ ìƒ˜í”Œì˜ ë¶„í¬ë„ ì‚¬ì´ë¥¼ êµ¬ë³„**í•˜ê²Œ ë˜ëŠ” ê³¼ì •ì„ ë‚˜ì—´í•œ ê²ƒì´ë‹¤.

í•˜ë‹¨ì˜ ìˆ˜í‰ì„ ì€ ê· ë“±í•œ ë…¸ì´ì¦ˆì¸ $z$ì˜ ì •ì˜ì—­ì— ëŒ€í•œ ê²ƒì´ê³ , ìƒë‹¨ì˜ ìˆ˜í‰ì„ ì€ $x$ì˜ ì •ì˜ì—­, ì¦‰ ë°ì´í„°ì˜ ì •ì˜ì—­ì— ëŒ€í•œ ê²ƒì´ë‹¤.

ìœ„ìª½ì„ í–¥í•œ í™”ì‚´í‘œëŠ” $x=G(z)$ê°€ ì–´ë–»ê²Œ non-uniformí•œ ë¶„í¬ë„ $p_g$ê°€ ë³€í™˜ëœ ìƒ˜í”Œë¡œ ê°•ì œí•˜ëŠ”ì§€ë¥¼ ë³´ì—¬ì¤€ë‹¤.

$G$ëŠ” $p_g$ì—ì„œ ê³ ë°€ë„ì˜ êµ¬ì—­ì—ì„œ ìˆ˜ì¶•í•˜ê³  low densityì˜ êµ¬ì—­ì—ì„œ í™•ì¥í•œë‹¤.

1. **ìˆ˜ë ´ì— ê°€ê¹Œìš´ ëŒ€ë¦½ ìŒì„ ê³ ë ¤í•˜ë©´, ****$p_g$****ëŠ” ****$p_{data}$****ì™€ ë¹„ìŠ·í•˜ê³  ****$D$****ëŠ” ë¶€ë¶„ì ìœ¼ë¡œ ì •í™•í•œ ë¶„ë¥˜ê¸°ì´ë‹¤.**
1. **ì•Œê³ ë¦¬ì¦˜ì˜ ë°˜ë³µ ê³¼ì •ì—ì„œ ****$D$****ëŠ” ë°ì´í„°ë¡œë¶€í„° ìƒ˜í”Œì„ êµ¬ë³„í•˜ê¸° ìœ„í•´ì„œ í›ˆë ¨ë˜ë©°, ****$D^*(x)={p_{data}(x)\over p_{data}(x)+p_g(x)}$****ë¡œ ìˆ˜ë ´ëœë‹¤.**
1. **$G$****ê°€ ê°±ì‹ ëœ í›„, ****$D$****ì˜ ê·¸ë ˆë””ì–¸íŠ¸ëŠ” *****data*****ë¡œ ë¶„ë¥˜ë  ê°€ëŠ¥ì„±ì´ ë†’ì€ *****region*****ìœ¼ë¡œ í˜ëŸ¬ê°€ë„ë¡ ì¸ë„í•œë‹¤.**
1. **ì—¬ëŸ¬ í›ˆë ¨ ìŠ¤í… í›„, ****$G$****ì™€ ****$D$****ê°€ ì¶©ë¶„í•œ ìˆ˜ìš©ë ¥ì„ ê°€ì§€ê²Œ ë˜ì—ˆë‹¤ë©´, ****$p_g=p_{data}$****ê°€ ë˜ì–´ *****discriminator*****ê°€ ë‘ *****distribution*****ë¥¼ êµ¬ë³„í•˜ì§€ ëª»í•˜ê²Œ ëœë‹¤.**

# Theoretical Results


---

**ì•Œê³ ë¦¬ì¦˜ 1 **ì ëŒ€ì  ìƒì„± ì‹ ê²½ë§ì˜ ë¯¸ë‹ˆ ë°°ì¹˜ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°• í›ˆë ¨. í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì¸ $k$ëŠ” *discriminator*ì— ì ìš©í•  ìŠ¤í… ìˆ˜ì´ë‹¤. ì—¬ê¸°ì„œëŠ” ì‹¤í—˜ì—ì„œ ê°€ì¥ ì €ë ´í•œ ì„¤ì •ì¸ $k=1$ì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤.

---

**í›ˆë ¨ ê³¼ì • ìˆ˜ë§Œí¼ ë°˜ë³µí•œë‹¤:**

**end for**

ê·¸ë ˆë””ì–¸íŠ¸ ê¸°ë°˜ì˜ ê°±ì‹ ì€ ì–´ë–¤ ê·¸ë ˆë””ì–¸íŠ¸ ê¸°ë°˜ í•™ìŠµ ê·œì¹™ì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

ìš°ë¦¬ëŠ” ëª¨ë©˜í…€ì„ ì‹¤í—˜ì—ì„œ ì‚¬ìš©í•˜ì˜€ë‹¤.

---

## Global Optimality of $p_g=p_{data}$

**Proposition 1. **ê³ ì •ëœ $G$ì— ëŒ€í•˜ì—¬ *optimal discriminator **$D$**ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.*

$$
D^*_G(x)={p_{data}(x)\over p_{data}(x)+p_g(x)}
$$

*ì¦ëª… 1. *ì ëŒ€ì  ìƒì„± ì‹ ê²½ë§ì˜ ë¹„ìš© í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\underset{G}{min}\underset{D}{max}V(D,G)=\mathbb{E}_{x\sim p_{data}(x)}[logD(x)]+\mathbb{E}_{z\sim p_z(z)}[log(1-D(G(z)))]
$$


$z$ë¥¼ $p_z(z)$ì—ì„œ ìƒ˜í”Œë§í•˜ì—¬ $G$ì— ì£¼ê³  $G(z)$ê°€ ìƒì„±í•œ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ $x$ë¼ê³  í•˜ì˜€ì„ ë•Œ $p_g(x)$ë¥¼ ë”°ë¥´ê²Œ ëœë‹¤. ì¦‰  í•´ë‹¹ ë¹„ìš© í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë°”ê¿€ ìˆ˜ ìˆë‹¤.

$$
\mathbb{E}_{x\sim p_{data}(x)}[logD(x)]+\mathbb{E}_{x\sim p_g(x)}[log(1-D(x))]
$$


í™•ë¥  ë°€ë„ í•¨ìˆ˜ $p(x)$ì— ê¸°ë°˜í•œ $f(x)$ì˜ ê¸°ëŒ“ê°’ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\mathbb{E}_{x \sim p(x)}[f(x)]=\int_xp(x)f(x)dx
$$

í•´ë‹¹ ìˆ˜ì‹ì„ ì´ìš©í•˜ì—¬ ë¹„ìš© í•¨ìˆ˜ë¥¼ ë³€í˜•í•˜ë©´

$$
 V(G, D) =\int_x p_{data}(x)logD(x)dx + \int_x p_g(x)log(1-D(x))dx
$$

$$
=\int_x p_{data}(x)log(D(x)) + p_g(x)log(1-D(x))dx
$$

ê²°êµ­ í•´ë‹¹ ì ë¶„ì‹ì„ ìµœëŒ€í™” í•´ì•¼ ëœë‹¤ëŠ” ê²°ë¡ ì´ ë‚˜ì˜¨ë‹¤.

$$
\underset{D}{max}V(G, D)=\int_x p_{data}(x)log(D(x))+p_g(x)log(1-D(x))dx
$$

ì—¬ê¸°ì„œ, $a=p_{data}(x), b=p_g(x)$ë¼ê³  í•˜ê³  $y=D(x)$ë¼ê³  í•˜ë©´,

$$
alog(y)+blog(1-y)
$$

ì„ ìµœëŒ€í™” í•´ì•¼ í•œë‹¤. 


ì‹ì„ ë¯¸ë¶„í•˜ì—¬ 0ì´ ë˜ëŠ” ì§€ì ì„ ì°¾ìœ¼ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
{a\over y}+{b\over 1-y}= 0
$$

$$
a(y-1)=by
$$

$$
y(a+b)=a
$$

$$
y={a\over a+b}
$$

$y={a\over a+b}$ì—ì„œ ìµœëŒ€í™”ê°€ ë˜ëŠ” ì§€ì ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.

$D$ì˜ í›ˆë ¨ ëª©ì ì€ $p_{data}$ì—ì„œ êµ¬í•œ $x$ì— ëŒ€í•˜ì—¬ ì¡°ê±´ë¶€ í™•ë¥  $P(Y=1|x)$ ë˜ëŠ” $p_g$ì—ì„œ êµ¬í•œ $x$ì— ëŒ€í•˜ì—¬ ì¡°ê±´ë¶€ í™•ë¥  $P(Y=0|x)$ì˜ *log-likelihood*ë¥¼ ìµœëŒ€í™” í•˜ëŠ” ê²ƒì´ë‹¤.

ë¹„ìš© í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë³€í™˜ ë  ìˆ˜ ìˆë‹¤.

$$
C(G)=\underset{D}{max}V(G,D)=\mathbb{E}_{x\sim p_{data}}\begin{bmatrix}{log{p_{data}(x)\over p_{data}(x)+p_g(x)}}\end{bmatrix}+\mathbb{E}_{x\sim p_{g}}\begin{bmatrix}{log{p_{g}(x)\over p_{data}(x)+p_g(x)}}\end{bmatrix}
$$


**Theorem 1. ****$C(G)$**ì˜ global minimumì€ $p_g=p_{data}$ì¼ë•Œ ì–»ì„ ìˆ˜ ìˆë‹¤. ê·¸ë•Œ $C(G)$ëŠ” $-log4$ì˜ ê°’ì„ ì–»ëŠ”ë‹¤.

*ì¦ëª… 2*. 

$C(G)$ ë¥¼ ì ë¶„ì‹ìœ¼ë¡œ ë³€í˜•í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
C(G)=\int_x p_{data}(x)log\begin{pmatrix}{p_{data}(x)\over p_{data}(x)+p_g(x)}\end{pmatrix}+p_g(x)log\begin{pmatrix}{p_g(x)\over p_{data}(x)+p_g(x)}\end{pmatrix}dx
$$

ìš°ë³€ì— $-log4 + log4$ë¥¼ ë”í•´ì¤€ë‹¤.

$$
=-log4+log4+\int_x p_{data}(x)log\begin{pmatrix}{p_{data}(x)\over p_{data}(x)+p_g(x)}\end{pmatrix}+p_g(x)log\begin{pmatrix}{p_g(x)\over p_{data}(x)+p_g(x)}\end{pmatrix}dx
$$

$log4=2log2$ì´ê³  $E(aX+b)=aE(X)+b$ ì´ë¯€ë¡œ,

$$
=-log4+\int_x p_{data}(x)log\begin{pmatrix}{2\cdot p_{data}(x)\over p_{data}(x)+p_g(x)}\end{pmatrix}+p_g(x)log\begin{pmatrix}{2\cdot p_g(x)\over p_{data}(x)+p_g(x)}\end{pmatrix}dx
$$

Kullback-Leibler Divergenceì˜ ì—°ì†í™•ë¥ ë¶„í¬ì— ëŒ€í•œ ì •ì˜ëŠ” $D_{KL}(B||A)=\int_xB(x)log({B(x)\over A(x)})dx$ ì´ë¯€ë¡œ,

$$
=-log4+D_{KL}(p_{data}(x)||{p_{data}(x)+p_g(x)\over2}+D_{KL}(p_g(x)||{p_{data}(x)+p_g(x)\over2})
$$

ì´ëŠ” Jensen-Shannon Divergenceì— ì˜í•˜ì—¬ $JSD(B||A)={\frac 1 2}D_{KL}(B||M)+{1\over2}D_{KL}(A||M)$ ì´ë¯€ë¡œ

$$
=-log4+2\cdot JSD(p_{data}(x)||p_g(x))
$$

ì´ë ‡ê²Œ ë³€í™˜ ë  ìˆ˜ ìˆë‹¤.

*Jensen-Shannon Divergence*ëŠ” symmetric í•˜ê¸° ë•Œë¬¸ì— í•­ìƒ non-negativeí•˜ë‹¤. ë”°ë¼ì„œ $C^*=-log(4)$ì´ $C(G)$ì˜ global minimum ì´ë‹¤.


## Convergence of Algorithm 1

***Proposition 2.***** **[**ì•Œê³ ë¦¬ì¦˜ 1**](/f6b0e5e737a6473f8acfe57a38b6c058#4947b9222411485d9e453025225fbb59)ì—ì„œ** **ê° ìŠ¤í…ë§ˆë‹¤ ë§Œì•½ Gì™€ Dê°€ ì¶©ë¶„í•œ ìˆ˜ìš©ë ¥ì„ ê°€ì§€ê³  ìˆë‹¤ë©´, íŒë³„ì *(discriminator)*ëŠ” 

# References

[https://arxiv.org/pdf/1406.2661.pdf](https://arxiv.org/pdf/1406.2661.pdf)

ì¤‘ìš”ìƒ˜í”Œë§ -  [https://pasus.tistory.com/52](https://pasus.tistory.com/52)

[https://memesoo99.tistory.com/27](https://memesoo99.tistory.com/27)

KLD - [https://datascienceschool.net/02%20mathematics/10.03%20%EA%B5%90%EC%B0%A8%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EC%99%80%20%EC%BF%A8%EB%B0%B1-%EB%9D%BC%EC%9D%B4%EB%B8%94%EB%9F%AC%20%EB%B0%9C%EC%82%B0.html](https://datascienceschool.net/02%20mathematics/10.03%20%EA%B5%90%EC%B0%A8%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EC%99%80%20%EC%BF%A8%EB%B0%B1-%EB%9D%BC%EC%9D%B4%EB%B8%94%EB%9F%AC%20%EB%B0%9C%EC%82%B0.html)

KLD, JSD - [https://hyeongminlee.github.io/post/prob002_kld_jsd/](https://hyeongminlee.github.io/post/prob002_kld_jsd/)

