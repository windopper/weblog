---
title: "An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale"
description: ""
tags: ["ML","Paper Review"]
date: "2023-01-21"
thumbnail: "/markdown-memo/legacy/images/d85a2e9b-ff58-4682-92f6-41794756e2f9.png"
---

<!-- Table of Contents -->

# Introduction

셀프 어텐션 기반 아키텍쳐들, 특히 **트랜스포머**는 자연어 처리 태스크에서 매우 뛰어난 성능을 보였다. 트랜스포머의 효율성과 확장성 덕분에 수억개의 파라미터의 개수를 가진 모델을 훈련시키는 것이 가능해졌다.

그러나 컴퓨터 비전에서는 **컨볼루션 아키텍처**가 여전히 주로 쓰이고 있다.

자연어 처리의 성공에 힘입어 **셀프 어텐션**과 **합성곱 신경망 아키텍처**를 결합하거나 **완전히 대체**하는 여러 작업들이 시도되었다.

가장 최근의 모델은 이론적으로 효율적이지만 *specialized attention patterns*의 사용 때문에 모던 하드웨어의 가속기에 의하여 효과적으로 scaled 되지 않는다는 문제가 있었다.

그래서 큰 스케일의 이미지 인식에서 기본적인 ResNet 류의 아키텍처가 여전히 SOTA이다.

자연어 처리 분야에서의 변종 트랜스포머에 영감을 받아, 해당 논문에서는 가능한 적게 변형을 준 기본적인 트랜스포머 모델을 직접 이미지에 적용하여 실험하였다.

그렇기 하기 위해서, **이미지를 여러 패치로 나눈 후** 해당 패치의 ***sequence of linear embedding***를 만들어 트랜스포머 모델의 입력으로 주었다.

이미지 패치들은 **자연어 처리 분야의 토큰과 같은 방식**으로 취급된다.


![](/markdown-memo/legacy/images/d85a2e9b-ff58-4682-92f6-41794756e2f9.png)

# Method

모델의 구조는 가능한 한 오리지널 트랜스포머의 구조를 따른다.

의도적으로 간단하게 구성한 해당 구조의 이점은 자연어 처리 트랜스포머 아키텍처의 확장성이 있고 모델의 효율적인 구현은 별도의 추가적인 구성 없이 사용될 수 있다.

## Vision Transformer

Vision Transformer(약칭 ViT)은 다음과 같이 작동된다.

1. **이미지를 ****고정된 크기의 패치****들로 나눈 후,**** 1차원 벡터로 *****flatten***** 시킨다.**
1. ***trainable linear projection*****을 통하여 ****$D$****차원으로 변환****한다. 해당 *****projection*****은 *****patch embeddings*****라고 부르기로 한다.**
1. **BERT의 클래스 토큰과 비슷하게, ****임베딩 패치 시퀀스 앞에 학습가능한 임베딩 벡터****를 연결한다. 이는 트랜스포머 인코더의 결과에서 *****image representation*****로서 제공되는 상태이다.**
1. ***patch embeddings*****에 *****learnable 1D***** *****position embeddings*****를 추가한다.**
1. **트랜스포머 인코더를 ****$L$****개를 쌓는다. Layer Normalization 층을 모든 블록 이전에, 잔차 연결을 모든 블록 이후에 추가한다.**

![](/markdown-memo/legacy/images/671a5551-302b-4dc0-874f-c76398e07694.png)


### Inductive bias

우리는 ViT가 CNN보다 덜 **image-specific inductive bias** 하다고 말한다.

CNN에서는** *****locality****, ****two-dimensional neighborhood structure**** *그리고 ***translation equivariance***가 전체 모델을 통하여 각 층에서 만들어 진다.

ViT에서는 ***self-attention***** 레이어가 *****global***하기 때문에 오직 MLP 레이어만이 ***local ***그리고 ***translationally equivariant***하다.

모델의 시작 부분에서 이미지를 패치로 나누고 미세 조정 단계에서 각각 다른 해상도의 이미지에 대한* position embeddings*를 조절하기 위해 *two-dimensional neighborhood structure*은 매우 관대하게 사용된다.

다른 것 보다, 초기 단계에서의* position embeddings*는 **패치들의 2D 위치에 대한 어떠한 정보를 가지고 있지 않고** 패치 사이의 모든 공간 관계는 처음부터 학습되어야 한다.

### Hybrid Architecture

이미지 패치들의 대안으로 **입력 시퀀스는 CNN의 피쳐 맵**으로 구성될 수 있다.

하이브리드 모델에서 *patch embedding projection* $E$는 **CNN 피처 맵으로부터 추출된 패치**에 적용된 것이다.

특별한 경우로 패치들은 **1X1의 공간 크기**를 가질 수 있는데, 이것은 **피쳐 맵의 공간 차원을 *****flattening***** **하고 **트랜스포머 차원으로 *****projecting ***함으로써 얻어진 입력 시퀀스를 의미한다.


## Fine-tuning And Higher Resolution

형식적으로, 우리는 큰 데이터 셋으로 ViT를 사전 훈련시키고 작은 태스크를 해결하기 위해 미세 조정을 한다.

이 작업을 위해 우리는 사전 훈련시에 사용했던 예측 레이어를 제거하고 zero-initialized된 $D\times K$의 피드포워드 레이어를 추가하였다. 여기서 $K$는 태스크의 클래스 수이다.

대개 사전 훈련 때보다 높은 해상도에서 미세 조정하는 것이 더 장점이 많다.

높은 해상도로 이미지를 통과시키면, 더 크고 효과적인 시퀀스 길이에서 패치 크기를 유지하게 되는 결과가 나온다.

ViT는 **메모리의 한계에 도달할 떄 까지 임의의 시퀀스 길이**를 다룰 수 있는데, 그러면 사전 학습된 ***position embeddings***는 의미가 없어지게 된다.

그러므로 우리는 원래 이미지의 위치에 따라 사전 학습된 **position embedding**의 **2D interpolation**을 수행한다.

해상도의 조절과 패치 추출은 이미지의 2D 구조에 대한 inductive bias가 ViT에 수동으로 주입되는 유일한 지점이다.


# Experiments

우리는 ResNet, ViT 그리고 하이브리드의 표현 학습 가능성을 평가한다.

각 모델의 데이터 필요를 이해하기 위해, 우리는 다양한 크기의 데이터 셋으로 사전 학습시키고 많은 벤치마크 태스크들로 평가한다.

모델을 사전 학습시키는 계산 비용을 고려할때, ViT는 매우 순조롭게(favourably) 작동되며 낮은 사전 훈련 비용으로 대부분의 recognition benchmark들에서 SOTA를 획득하였다.

마지막으로 우리는 자기 지도 학습을 사용하여 작은 실험을 진행했고 이는 **자기 지도 ViT가 미래에 성공할 가능성**이 있다는 것을 보여준다.

# Setup

## Datasets

사전 학습시킨 데이터 → ILSVRC-2012 ImageNet(여기서는 그냥 ImageNet이라고 부르기로 한다), ImageNet의 상위 집합인 ImageNet-21k, JFT

전이 학습시킨 데이터 → ImageNet, ImageNet-ReaL, CIFAR-10/100, Oxford-IIIT Pets, Oxford Flowers-102.

우리는 또한 19-task VTAG 분류 모음들로 평가한다. VTAB은 각 태스크마다 1000개의 학습 예시들을 사용하여 적은 데이터들을 다양한 태스크에 전이하여 평가한다.

태스크들은 세가지 그룹으로 나뉜다.

- Natural - Pets, CIFAR 데이터와 같읕 것들
- Specialized - 의학이나 위성 사진들
- Structured - localization과 같은 geometric understanding이 필요한 것들

## Model Variants

![](/markdown-memo/legacy/images/bbf21ce4-947a-467d-8f02-d5d10ebc1efb.png)



# Implementation

<!-- Unknown block type: link_to_page -->

# References

[https://arxiv.org/pdf/2010.11929v2.pdf](https://arxiv.org/pdf/2010.11929v2.pdf)

[https://re-code-cord.tistory.com/entry/Inductive-Bias란-무엇일까](https://re-code-cord.tistory.com/entry/Inductive-Bias%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%BC%EA%B9%8C)

