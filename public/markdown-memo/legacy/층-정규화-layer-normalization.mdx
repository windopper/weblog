---
title: "층 정규화 (Layer Normalization)"
description: ""
tags: ["ML","Tutorial","Knowledge"]
date: "2023-01-20"
thumbnail: ""
---

# 개요

층 정규화는 **텐서의 마지막 차원**에 대해서 평균과 분산을 구하고, 이를 가지고 어떤 수식을 통해 값을 정규화하는 기법입니다.

사실 상 배치 정규화랑 효과는 똑같으며 정규화 하는 대상이 다를 뿐입니다.

배치 정규화는 모든 샘플에 대한 각 피쳐들을 대상으로 한다면 층 정규화는 각 샘플에 대한 피쳐들을 대상으로 한다는 점이 다릅니다.


층 정규화 수식

벡터 $x_i$의 각 $k$차원의 값이 다음과 같이 정규화 된다.

### $   \hat{x}_{i,k}={{x_{i,k}-\mu_i}\over{\sqrt{\sigma^2_i+\epsilon}}}$


$x_i$ → 벡터

평균 $\mu_i$ → 스칼라

분산 $\sigma^2_i$ → 스칼라

$\epsilon$은 분모가 0이되는 것을 방지하는 값


이제 $\gamma$와 $\beta$라는 벡터를 준비한다. 단, 이들의 초기 값은 각각 1과 0이다.

$\gamma = [1, 1, 1, 1...]$

$\beta=[0, 0, 0, 0...]$


$\gamma$와 $\beta$를 도입한 층 정규화의 최종 수식은 다음과 같으며, $\gamma$와 $\beta$는 학습 가능한 파라미터이다.


$ln_i=\gamma\hat{x}_i+\beta=LayerNorm(x_i)$


케라스에서는 층 정규화를 위한 LayerNormaliztion()을 제공


# Numpy 구현

추가 예정 :)




