---
title: "Image Style Transfer Using Convolutional Neural Network"
description: ""
tags: ["Pytorch","Style Transfer","ML","Paper Implementation"]
date: "2023-01-20"
thumbnail: "/markdown-memo/legacy/images/efcdef56-197b-4764-b2a4-66e7215e056e.png"
---


[https://colab.research.google.com/drive/1IlEmPsHzXIGYM6qGKLfj41Ev08FLyRPv?usp=sharing](https://colab.research.google.com/drive/1IlEmPsHzXIGYM6qGKLfj41Ev08FLyRPv?usp=sharing)


<!-- Table of Contents -->


```python
import torch
import torchvision
import matplotlib.pyplot as plt
from PIL import Image
import copy
from urllib.request import urlopen
```

# 유틸 함수 정의

```python
# 이미지로더
# 이미지의 크기를 변경 후 텐서로 변환
loader = torchvision.transforms.Compose([
	torchvision.transforms.Resize((512, 640)),
	torchvision.transforms.ToTensor(),
])

def image_loader(image_name):
	image = Image.open(image_name)
	image = loader(image).unsqueeze(0)
	# 신경망에 들어가는 입력 데이터는 배치 사이즈 축을 가지고 있으므로
  # unsqueeze() 메서드로 배치 축을 늘려준다.
  # 예를 들어, (3, 512, 640) 크기의 이미지가 들어왔다면
  # (1, 3, 512, 640) 로 변환하여 신경망에 들어갈 수 있도록 바꾼다
	return image.to(device, dtype=torch.float)
	# 이미지를 device에 올려 매개변수들이 같은 하드웨어 메모리 상에 있도록 해준다

def imshow(x):
  image = x.cpu().clone()
  # cpu로 올린 이미지를 복제한다.
  image = image.squeeze(0)
  # 해당 함수에서 받은 x값은 배치 축이 적용된 이미지라고 가정하기 때문에
  # 배치 축을 없애서 matplotlib.pyplot 의 imshow 메서드가 입력 값을 받을 수 있도록 조정한다.
  image = torchvision.transforms.ToPILImage()(image)
  # 텐서 형태의 이미지를 PILImage로 변환해준다.
  plt.imshow(image)
  plt.show()

# 그람 행렬 정의
def gram_matrix(x):
	b, c, h, w = x.size()
	features = x.view(b*c, h*w)
	return torch.mm(features, features.T).div(b*c*h*w)
```

# 백본 네트워크 가져오기

```python
cnn = torchvision.models.vgg19(pretrained=True).features.to(device).eval()
```


# Normalization 정의

```python
cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)
cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)

class Normalization(torch.nn.Module):
  def __init__(self, mean, std):
    super(Normalization, self).__init__()
    self.mean = mean.clone().view(-1, 1, 1)
    self.std = std.clone().view(-1, 1, 1)

  def forward(self, img):
    return (img - self.mean) / self.std
```

# Style Loss 정의

```python
class StyleLoss(torch.nn.Module):
	def __init__(self, target):
		super(StyleLoss, self).__init__()
		self.target = gram_matrix(target).detach()

	def forward(self, input):
		g = gram_matrix(input)
		self.loss = torch.nn.funcitonal.mse_loss(g, self.target)
		return input
```

# Content Loss 정의

```python
class ContentLoss(torch.nn.Module):
	def __init__(self, target):
		super(ContentLoss, self).__init__()
		self.target = target.detach()

	def forward(self, input):
		self.loss = torch.nn.functional.mse_loss(input, self.target)
		return input
```

# Style Transfer 시작하기

```python
# Style Transfer 시작하기
# 이전에 만들었던 style loss 와 content loss 를 혼합하여 스타일 트랜스퍼링을 시작
content_layers = ['conv_4']
style_layers = ['conv_1', 'conv_3', 'conv_5', 'conv_7', 'conv_9']

def get_losses(cnn, content_img, style_img, noise_img):
  cnn = copy.deepcopy(cnn)
	normalization = Normalization(cnn_normalization_mean, cnn_normalization_std)
  content_losses = []
  style_losses = []

	# 모델의 레이어를 순회하며 style loss와 content loss층을 포함한 새로운 모델을 만들 것이므로
	# Sequential을 사용하여 모델을 생성
  model = torch.nn.Sequential(normalization)

  i = 0

	# 백본 네트워크에서 레이어를 순회한다.
  for layer in cnn.children():
    if isinstance(layer, torch.nn.Conv2d):
      i+=1
      name = 'conv_{}'.format(i)
    elif isinstance(layer, torch.nn.ReLU):
      name = 'relu_{}'.format(i)
      layer = torch.nn.ReLU(inplace=False)
    elif isinstance(layer, torch.nn.MaxPool2d):
      name = 'pool_{}'.format(i)
    elif isinstance(layer, torch.nn.BatchNorm2d):
      name = 'bn_{}'.format(i)
    else:
      raise RuntimeError('Unrecognize layer: {}'.format(layer.__class__.__name__))
		
    model.add_module(name, layer)
		
		# 지정한 레이어 이름에 포함되어 있다면 해당 합성곱 신경망을 통과한 피쳐 값을 이용하여
		# 손실 함수을 만들고 새로운 모델에 추가한다.
		# 또한 손실 함수 리스트에 추가시켜 필요 할 때 손실값에 접근 할 수 있도록 한다.
    if name in content_layers:
      feature = model(content_img).detach()
      content_loss = ContentLoss(feature)
      model.add_module('content_loss_{}'.format(i), content_loss)
      content_losses.append(content_loss)

    if name in style_layers:
      feature = model(style_img).detach()
      style_loss = StyleLoss(feature)
      model.add_module('style_loss_{}'.format(i), style_loss)
      style_losses.append(style_loss)
	
	# 생성한 모델을 거꾸로 순회하며 우리가 정의한 손실 함수들을 만난다면 순회를 멈춘다.
	# 이때 i 값은 마지막 손실 함수 레이어에 위치한다.
  for i in range(len(model) - 1, -1, -1):
    if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):
      break

	# 인덱스 슬라이싱을 통하여 모델의 마지막 층이 손실 함수이도록 한다.
	# 우리는 백본 신경망에서 입력 이미지의 특정 레이어에서의 피쳐 값만을 필요로 한다. 
	# 즉, 그 뒤의 피쳐 값은 필요가 없으므로 불필요한 연산을 막기 위해 레이어를 삭제한다.
  model = model[:(i+1)]

  return model, content_losses, style_losses

"""
스타일 트랜스퍼를 진행하는 함수
"""
def style_transfer(cnn, content_img, style_img, input_img, iters):
  model, content_losses, style_losses = get_losses(cnn, content_img, style_img, input_img)
	# 옵티마이저로 LBFGS를 사용한다.
	# 논문에서 해당 옵티마이저를 사용 했을 때 가장 좋은 성과를 보였다고 한다.
  optimizer = torch.optim.LBFGS([input_img.requires_grad_()])

  print('Start')
  imshow(input_img)

  run = [0]
  while run[0] <= iters:
    def closure():
			# 입력 이미지의 값 범위를 0과 1사이로 강제한다.
			# 0보다 작으면 0으로 1보다 크면 1로 변환된다.
      input_img.data.clamp_(0, 1)
			# 옵티마이저의 그레디언트를 초기화 한다.
      optimizer.zero_grad()
			# 입력 이미지를 모델에 통과시킨다.
      model(input_img)

      style_score = 0
      content_score = 0

      for sl in style_losses:
        style_score += sl.loss
      for cl in content_losses:
        content_score += cl.loss

      style_score *= 1e5
      loss = content_score + style_score
      loss.backward()

      run[0] += 1
      if run[0] % 100 == 0:
        print(f'Epoch: {run[0]} / Content Loss: {content_score.item()} / Style_loss: {style_score.item()}')
        imshow(input_img)

      return content_score + style_score

		# LBFGS 옵티마이저의 경우 여러 번 모델이 재평가 된다.
		# 따라서 클로져 함수를 통해 모델을 다시 계산할 수 있도록 한다.
    optimizer.step(closure)

  input_img.data.clamp_(0,1)

  return input_img
```


# 스타일 트랜스퍼 결과

![](/markdown-memo/legacy/images/efcdef56-197b-4764-b2a4-66e7215e056e.png)

컨텐츠 이미지로 높은 빌딩을 아래에서 바라보는 사진을 선택했다.

![](/markdown-memo/legacy/images/ad0a8d2d-e0ec-4ef0-a2db-afc02af138e3.png)

스타일 이미지로 산을 선의 형태로 추상적인 배경을 표현한 그림을 선택했다.

![](/markdown-memo/legacy/images/1ffd798b-3ed9-44ab-9b6a-e5eaae9975ba.png)

스타일 트랜스퍼 결과이다.

기존 컨텐츠에 스타일이 예쁘게 적용되었다!


