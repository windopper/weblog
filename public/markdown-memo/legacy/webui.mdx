---
title: "Webui"
description: ""
tags: ["Stable Diffusion"]
date: "2024-01-11"
thumbnail: ""
---

<!-- Table of Contents -->


# WebUI Setting

SDXL 모델 실행시 webui-user.bat의 COMMANDLINE_ARGS에 —no-half-vae를 활성화해야한다.

모델 구동시 vram 부족시 COMMANDLINE_ARGS에 —lowvram 또는 —medvram 옵션을 켜준다 

# 이미지 재현

동일한 이미지 생성을 재현하기 위해서 **동일한 그래픽 카드**가 필요하다.

만약 다른 그래픽 카드로 같은 세팅의 이미지를 생성하면 비슷하지만 다른 결과물이 나온다. 이를 보완하기 위해서는 CPU Render를 하거나 (CPU Render는 속도가 매우매우 느리지만 기종 상관없이 같은 결과물을 출력) 난수 생성기를 CPU에서 사용해야한다.* *(하지만 그래픽 카드가 다르면 아주 미세한 차이가 남)

# 이미지 생성 속도

## —xformers vs —opt-sdp-attention --opt-sdp-no-mem-attention

생성 속도를 높이고 vram 사용량을 줄임.

근소한 차이로 --opt-sdp-attention --opt-sdp-no-mem-attention 를 선호함.

## —no-half

희생된 정밀도를 복구하여 이미지 재현율을 높임. vram 에 부담을 주고 속도를 저하시킴.

# 그래서 제일 좋은 옵션은?

## 일반적인 모델

1. 동일 그래픽 카드 + —opt-sdp-attention --opt-sdp-no-mem-attention
1. CPU Seed + --opt-sdp-attention --opt-sdp-no-mem-attention (8gb 이하)
1. GPU Seed + --opt-sdp-attention --opt-sdp-no-mem-attention (8gb 이하)
1. CPU Seed + —xformers
1. GPU Seed + —xformers
## SDXL 모델 사용시

1. 4090 RTX —xformers —no-half-vae
1. 동일 그래픽 카드 + --opt-sdp-attention --opt-sdp-no-mem-attention —no-half-vae
1. CPU Seed + --opt-sdp-attention --opt-sdp-no-mem-attention —no-half-vae —medvram (8gb 이하)
1. GPU Seed + --opt-sdp-attention --opt-sdp-no-mem-attention —no-half-vae —medvram (8gb 이하)
# T2I

## 자주 쓰는 기술

- T2I에서 I2I 인페인팅 기능으로 표정바꿀때
원본 시드와 프롬 + 표정 프롬 + Openpose + Canny (머리 부분 지우고) + Inpaint_only+lama

- 얼굴 각도 변경 (webui 1.6.0 이상부터)
원본 시드와 프롬 + reference only(원본 이미지) +  mediapipe_face (원하는 얼굴 각) + openpose + canny (머리 부분 지우고) 

# 프롬프팅

<!-- Unknown block type: child_page -->

# Seed와 Image Size

Seed를 고정한다고 하더라도 Image Size가 달라지면 초기 노이즈가 달라지기 때문에 같은 그림을 생성하지 않는다.

Image SIze를 과하게 키우면 비슷한 그림이 중첩되어 나오는 경우가 있을 수 있다.

따라서 이미지를 생성하기 전에 특정 사이즈를 정하고 가야한다. (512 x 512등)

# Seed Extras

hires, i2i랑 매우 비슷한 기능. 본래 해상도를 조정하면 다른 이미지를 생성하는 것을 최대한 억제하는 용도로도 사용가능하고 (outpaint 느낌으로 사용가능), variation strength를 조정하여 기존 생성된 이미지를 다른 이미지 처럼 보이도록 수정할 수 있다.

# Sampling Method

픽셀 공간의 노이즈를 줄여나가는 방법을 정의함. 일반적으로 Karras 모델이 좋은 이미지를 생성한다는 논문 결과가 있다고 함.

# Sampling Step & CFG

sampling step이 높아질수록 더 정교한 정보를 생성한다.

DPM++ 2M Karras 샘플러 기준

간단한 프롬프트 혹은 작은 이미지 20~40

복잡한 프롬프트 혹은 중간 크기의 이미지 40~60

그 이상 60~

cfg가 높아질 수록 더 많이 생각한다. 너무 높아지면 뇌절을 한다.

일반적으로 7~13 사이의 값으로 설정함.

# Batch Count 와 Batch Size

Batch Count는 한장씩 여러번 뽑는 거고, Batch Size는 여러장씩 한번에 뽑는 것.

차이점은 Batch Count는 원본과 완전히 동일하지만 Batch Size는 약간의 missing이 존재한다.

속도는 Batch Size가 Batch Count 보다 빠르다.

> 💡 vram이 100% 사용하는 경우 Batch Count가 Batch Size보다 빠르다.

# Hires.fix

생성된 이미지의 크기와 디테일을 향상시키는 방법 중 하나. 그러나 업스케일과는 다른 점으로는 생성된 이미지에 노이즈를 추가하여 변칙성을 줌으로써 오류가 나는 부분을 재해석 한다.

## 과정

1. 기존 이미지를 업스케일 한다.
1. 업스케일된 이미지와 같은 해상도, 같은 시드의 노이즈를 이미지로 생성하여 denoising strength 값 만큼 합침.

Hires Step → 선명도에 영향을 준다. 이미지가 실사에 가깝거나 복잡할 수록 사소한 디테일을 보정한다. 

resize시 원래 이미지의 업스케일된 사이즈와 맞지 않는다면 크롭하여 진행한다. 따라서 그림이 잘리는 경우가 있다.

# Restore face

흐릿한 얼굴을 복원함. anime 타입보다는 real 타입에 잘 적용되며 anime 타입에 restore face를 적용하면 real 타입의 얼굴로 복원함.

Codeformer

GFPGAN

# Dynamic Prompts

## Attention Graber

키워드들 중 랜덤하게 선택하여 지정한 값 사이의 어텐션으로 지정한다.

## Wildcard

미리 저장한 키워드들을 하나의 키워드로 저장할 수 있도록 한다.

# Prompt Preset

프롬프트를 미리 저장했다가 사용할 수 있는 기능. {prompt}를 통해 입력 프롬프트의 위치를 조절할 수 있다.

# I2I

## 자주 쓰는 기술

- 얼굴 표정 바꾸기
i2i inpaint + open pose + canny 얼굴 지운거. denoising strength 1

- 시선 변경
눈 부분 inpaint, 동공 리깅 후 lineart 에 넣기

- Hires 기능
동일 seed, 프롬프트, 원본 이미지를 삽입 후 denoising strength 0.35, cfg scale 3 정도로 맞춘 후 업스케일 진행

## CFG

text embedding으로 생성된 이미지의 개입을 조절할 수 있음. 단순 업스케일 시에 CFG 값이 높으면 이미지를 과하게 재해석하는 경향이 있음

## Resize Mode

Just Resize → 원래 이미지를 사이즈를 그냥 바꿔버림

Crop and resize → 원래 이미지에서 바꾸려고 하는 사이즈만큼 크롭하여 사이즈를 바꿈

Resize and fill → 사이즈를 바꾸고 빈공간을 이미지의 평균 색으로 채움

Just resize (latent upscale) → Just Resize와 유사하지만 Size 조정이 Latent space에서 수행됨.

# I2I inpaint

mask blur → 마스크의 경계를 부드럽게 함

mask mode → Inpaint 가 적용될 부분을 지정 

Inpaint area →

- Whole picture → 원본을 기준으로 계산.
- Only masked → 마스킹된 이미지를 크롭하여 재계산 후 적용.
자원(노이즈)를 더 확보한 후 unet에서 계산하므로 더 세세한 이미지가 생성됨.

## 부분 디테일 수정을 위한 단계

### T2I

1. step * cfg (xyz plot)
### I2I

1. Extra seed noise
1. seed noise
1. step * cfg (xyz plot)
# I2I Inpaint sketch

mask transparency → 수치가 낮을 수록 스케치한 색감을 최대한 따라감. 높을 수록 원본 색감을 따라감. 높을 수록 투명도가 높아지는 느낌.

# I2I Inpaint Upload

마스킹 파일을 가져와서 사용하는 인페인팅 방식. 동일한 이미지 재현이 가능해지며(동일한 마스킹 픽셀을 사용하기 때문에), 일반적으로 segment anything (이미지의 특정 부분을 분절할 수 있는 기능)과 연동하여 사용한다.

# I2I를 이용한 Upscaling

T2I의 Hires.fix 대신에 I2I에서 Upscaling을 할 때 주의사항으로 source 이미지 상황에 맞추어 Resolution을 조정해야 한다.

해상도가 너무 작다면 어텐션이 집중된 부분을 다 표현을 하지 못하고 결과물을 출력하고, 해상도가 너무 크다면 어텐션이 과하게 퍼져 과해석을 할 수 있다.

# Controlnet

## Basic Option

Pixel Perfect → 이미지 픽셀의 최적값으로 계산. 필요 vram 증가함.

## Control Type

### Canny

누끼를 따는 느낌.

### Lineart

이미지의 엣지를 감지함. 원화같은 그림을 표현할 때 많이 사용됨.

### SoftEdge

원본 이미지에서 엣지를 감지함. 노이즈가 적고 엣지 감지 결과가 좋아서 주로 사용함.

Canny나 Lineart에 비해서 세세한 디테일보다는 큰 범위에서의 shape을 따오기 때문에 원본 이미지와 과하게 표현되지 않는다.

### Scribble

이미지의 엣지를 감지하여 러프 스케치처럼 표현함.

scribble_hed → 사람과 같은 윤곽선을 생성하는 데 능숙

scribble_pidinet → hed와 유사, 세부정보가 적은 edge위주 line

scribble_xdog → 디테일한 edge 감지

> 💡 해상도가 낮으면 scribble 윤곽이 매우 굵게나온다. 그럴 때 I2I 탭에서 프롬프트 설정을 하지 않고 이미지의 해상도를 증가시킨 후 scribble 컨트롤넷을 설정하면 세세하게 나온다.

### MLSD

직선형태의 엣지를 감지한다. 인테리어, 건물, 거리 풍경, 액자 종이 모서리 등의 감지에 사용함.

### Depth

원본 이미지 요소들의 카메라에서의 거리(깊이감)을 감지함. Depth가 흰색일수록 가까운 거리에 있다고 보면됨.

depth_leres → Detail이 있고, 배경까지 감지

depth_leres++ → depth_leres보다 더 디테일

depth_midas → 오래된 depth 감지기. 일반적인 사용

depth_zoe → midas와 leres 중간 detail

### Normal

이미지에 있는 물체의 각 면의 방향을 감지함. 물체의 덩어리? 들을 확실하게 구분하는데 도움됨.

normal_midas → 캐릭터 위주의 normal

normal_bae → 이미지 전체의 normal (BG 포함)

### OpenPose

눈, 코, 목, 어깨, 팔꿈치, 손목, 무릎, 발목의 위치를 감지함. 포즈를 유지한채 다른 이미지를 생성할 때 주로 사용함.

### Media Pipe

얼굴 각도, 표정을 바꿀 수 있음. 별도 설치 필요.

### IP2P

대화형으로 이미지 수정을 지시함. 

### Tile

이미지의 새로운 세부 정보를 생성함.

Souce Tile의 의미 체계와 프롬프트가 불일치 할 경우, Prompt를 무시하고 로컬 컨텍스트로 세부 정보를 생성함.

### Reference

원본 이미지를 참조하여 새로운 이미지를 생성할 때 사용함.

### T2IA

Source Image를 Color Grid 또는 Clip Vision Embedding으로 변환하여 이미지를 생성함.

### Shuffle

무작위 흐름을 사용하여 이미지를 섞고 Stable Diffusion을 제어하여 이미지를 재구성함.

### Inpaint

I2I의 Inpaint tab에서 사용하며, 마스킹 부분 이미지 재생성 시 사용함.

> 💡 A1111의 Inpaint 기능과의 차이점은 A1111은 이미지보다는 프롬프트에 조금 더 치중하여 생성되다는 느낌이고, 컨트롤넷의 Inpaint는 이미지과의 연계성이 조금 더 있어보이도록 생성한다.

### Seg

원본 이미지에서 Object 종류를 학습되어 있는 CODE COLOR로 분류함.

### IPADAPTER

프롬프트를 사용하지 않아도 이미지를 분석하여 관련 스타일들을 그대로 남겨줄 수 있음.

### Controlnet Weight

컨트롤넷의 세기를 조절함.

## Starting control Step, Ending Control Step 

UNet의 레이어 중 어디부터 어디까지 컨트롤넷을 적용할 건지

## Control Mode

My prompt is more important → 프롬프트에 조금 더 가중치를 둔다. 컨트롤넷으로 설정한 것 외의 프롬프트를 설정한 것이 생성될 수 있다.

ControlNet is more important → 컨트롤넷에 조금 더 구속력을 둔다.

## Preprocessor

Preprocessor는 원본 이미지에서 컨트롤넷 모델을 적용하기 위한 Input Detected maps를 만들어 내는데 사용되는 부분이다.

## Resize Mode

I2I의 Resize Mode와 똑같음.

# Regional Prompter

이미지 구역별로 프롬프트를 적용할 수 있는 기술.

## Base Ratio

Base Prompt와 Region Prompt의 비율을 설정함.

## Use base prompt

기본 prompt를 모든 영역에서 사용함. 영역의 분할을 위해서는 기본 prompt에 BREAK 단어를 사용하여 구분함.

## Use Common prompt

Common Pormpt로 지정된 Prompt가 모든 영역에 동일하게 적용됨. 


# Adetailer

yolo등 감지기를 통해서 마스킹 된 부분을 재생성하여 디테일을 살려줌.

- 프롬프트에 로라를 넣을 수 있음. 손 생성 시 손 관련 네거티브 텍스쳐 인버전을 넣어 주는 방법이 있다.
- 손 생성 시 오픈포즈 컨트롤넷을 통해 더 정확한 손을 생성할 수 있다.
# Lora Block Weight

### 쓰는 이유??

Lora를 일부분에만 영향을 주고 싶을 때

Lora, Locon등을 사용할 때 적용되는 각 Block에 적용되는 가중치를 설정해주는 익스텐션

`<lora:”lora name”:1:lbw=가중치preset이름>` 형식으로 프롬프트 적용

[https://arca.live/b/aiart/72001338](https://arca.live/b/aiart/72001338)

# T2I vs I2I

T2I와 I2I의 기능이 겹치는 경우가 많은데 같은 기능으로 동일 이미지를 생성하였을 때 I2I가 조금 더 과하다는 생각이 들 수도 있다.

T2I와는 달리 I2I의 원본 이미지를 참조하는 과정이 한번더 추가되기 때문에 디퓨전 프로세스가 T2I보다는 더 많은 정보를 생성하기 때문이다.

# 


