---
title: "ì£¼íƒ ê°€ê²© ì˜ˆì¸¡: íšŒê·€ ë¬¸ì œ"
description: ""
tags: ["Tensorflow","Tutorial","ML"]
date: "2023-01-20"
thumbnail: "/markdown-memo/legacy/images/2f724ee4-e089-4ddc-ace9-6026574fc070.png"
---


ì£¼íƒ ê°€ê²© ì˜ˆì¸¡ì— í•„ìš”í•œ ë°ì´í„° ì…‹ì€ ì¼€ë¼ìŠ¤ì˜ datasets ëª¨ë“ˆì˜ boston_housing ì„ ì‚¬ìš©í–ˆë‹¤.

í•´ë‹¹ ë°ì´í„°ì…‹ì€ 1970ë…„ ì¤‘ë°˜ ë³´ìŠ¤í„´ ì™¸ê³½ ì§€ì—­ì˜ ë²”ì£„ìœ¨, ì§€ë°©ì„¸ìœ¨ ë“±ì˜ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì£¼íƒ ê°€ê²©ì˜ ì¤‘ê°„ ê°’ì„ ì˜ˆì¸¡í•´ ë³´ê² ë‹¤.

```python
from tensorflow.keras.datasets import boston_housing

(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()

"""
train_data[0]

array([  1.23247,   0.     ,   8.14   ,   0.     ,   0.538  ,   6.142  ,
        91.7    ,   3.9769 ,   4.     , 307.     ,  21.     , 396.9    ,
        18.72   ])

len(train_data)
404

len(test_data)
102

"""
```

í•´ë‹¹ ë°ì´í„° ì…‹ì€ ì´ 404ê°œì˜ í›ˆë ¨ ë°ì´í„°ì™€ 102ê°œì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.


# ë°ì´í„° ì „ì²˜ë¦¬

ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ê¸°ì— ì•ì„œ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•´ë³´ì.

ì…ë ¥ë°ì´í„°ë¥¼ í•œë²ˆ ì‚´í´ë³´ë©´ ìƒì´í•œ ìŠ¤ì¼€ì¼ì„ ê°€ì§„ ê°’ë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

ì´ëŸ¬í•œ ê°’ë“¤ì„ ëª¨ë¸ì— ê·¸ëŒ€ë¡œ ì£¼ì…í•˜ì—¬ í•™ìŠµì„ ì§„í–‰ì‹œí‚¬ ìˆ˜ë„ ìˆì§€ë§Œ í•´ë‹¹ ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ëŠë‚„ ìˆ˜ ìˆë‹¤.

ë”°ë¼ì„œ ì´ëŸ° ê°ê¸° ë‹¤ë¥¸ íŠ¹ì„±ì„ ê°€ì§„ ë°ì´í„°ë“¤ì„ íŠ¹ì„±ë³„ë¡œ ì •ê·œí™”ë¥¼ í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤.

```python
mean = train_data.mean(axis=0) # ì²«ë²ˆì§¸ ì¶•ì˜ ë°ì´í„°ì— ëŒ€í•œ í‰ê· ê°’ì„ ë‚¸ë‹¤.
train_data -= mean
std = train_data.std(axis=0) # ì²«ë²ˆì¬ ì¶•ì˜ ë°ì´í„°ì— ëŒ€í•œ í‘œì¤€ í¸ì°¨ë¥¼ ë‚¸ë‹¤.
train_data /= std

test_data -= mean # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ë˜‘ê°™ì´ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•œë‹¤.
test_data /= std
```


ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ ì‚¬ì´í‚·ëŸ°ì—ëŠ” í•´ë‹¹ ì „ì²˜ë¦¬ë¥¼ ì‰½ê²Œ ì§„í–‰í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í•¨ìˆ˜ê°€ ë§ˆë ¨ë˜ì–´ìˆë‹¤.

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(train_data)
train_data = scaler.transform(train_data)
test_data = scaler.transform(test_data)
```


> ğŸ’¡ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì •ê·œí™”í•  ë•Œ ì‚¬ìš©í•œ ê°’ì´ í›ˆë ¨ ë°ì´í„°ì—ì„œ ê³„ì‚°í•œ ê°’ì„ì„ ì•Œì•„ì•¼ í•œë‹¤. ì „ì²˜ë¦¬ ì‘ì—… ê³¼ì •ì—ì„œ ì ˆëŒ€ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ê³„ì‚°í•œ ì–´ë– í•œ ê°’ë„ ì‚¬ìš©í•´ì„œëŠ” ì•ˆëœë‹¤.


# ëª¨ë¸ êµ¬ì„±í•˜ê¸°

í•´ë‹¹ ë¬¸ì œëŠ” íšŒê·€ ë¬¸ì œì´ë¯€ë¡œ ë§ˆì§€ë§‰ ì¸µì˜ í™œì„±í™” í•¨ìˆ˜ë¡œ sigmoidë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.

ì˜µí‹°ë§ˆì´ì €ë¡œ rmsprop, ì†ì‹¤ í•¨ìˆ˜ë¡œ í‰ê·  ì œê³± ì˜¤ì°¨ (mse)ë¥¼ ì‚¬ìš©í•˜ì˜€ê³ , ë°ì´í„°ë¥¼ ëª¨ë‹ˆí„°ë§ í•  ì§€í‘œë¡œ í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (mae)ë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.

```python
from tensorflow import keras
from tensorflow.keras import layers

def build_model():
	model = keras.Sequential([
		layers.Dense(64, activation='relu'),
		layers.Dense(64, activation='relu'),
		layers.Dense(1, activation='sigmoid')
	])
	model.compile(
		optimizer='rmsprop',
		loss='mse',
		metrics=['mae']
	)
	return model
```


# K-Fold Validation ì„ ì´ìš©í•œ ëª¨ë¸ ê²€ì¦

í›ˆë ¨ì— ì‚¬ìš©í•  ë§¤ê°œë³€ìˆ˜ë“¤ì„ ì¡°ì •í•˜ë©° ëª¨ë¸ì„ í‰ê°€í•˜ê¸° ìœ„í•´ í›ˆë ¨ ë°ì´í„°ì—ì„œ ê²€ì¦ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•œë‹¤.

ê·¸ëŸ¬ë‚˜ í›ˆë ¨ ë°ì´í„°ì˜ ìˆ˜ê°€ ë§ì§€ ì•Šê¸° ë•Œë¬¸ì— ë°ì´í„°ë“¤ì´ í¸í–¥ì ìœ¼ë¡œ ë¶„ë¦¬ë  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.

ë”°ë¼ì„œ ë°ì´í„°ë¥¼ Kê°œì˜ ë¶„í• ë¡œ ë‚˜ëˆˆ í›„ í•´ë‹¹ ë¶„í• ì—ì„œ í›ˆë ¨í•˜ê³  ë‚˜ë¨¸ì§€ ë¶„í• ì—ì„œ ê²€ì¦í•˜ì—¬ ì´ Kë²ˆ í‰ê°€í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ìµœëŒ€í•œ ê³µì •í•˜ê²Œ í‰ê°€í•  ê²ƒì´ë‹¤.


```python
import numpy as np

K = 4
num_val_samples = len(train_data) // K
num_epochs = 100
all_scores = []

for i in range(K):
	print(f"preprocessing {i}'th fold")
	val_data = train_data[i*num_val_samples:(i+1)*num_val_samples]
	val_targets = train_targets[i*num_val_samples:(i+1)*num_val_samples]
	"""
	ê²€ì¦ ë°ì´í„° ë° íƒ€ê²Ÿ ë¶„í• 
	"""
	partial_train_data = np.concatenate([
		train_data[:i*num_val_samples],
		train_data[(i+1)*num_val_samples:]		
	])
	partial_train_targets = np.concatenate([
		train_targets[:i*num_val_samples],
		train_targets[(i+1)*num_val_samples:]
	])
	"""
	í›ˆë ¨ ë°ì´í„° ë° íƒ€ê²Ÿ ë¶„í• 
	"""
	model = build_model()
	model.fit(
		partial_train_data,
		partial_train_targets,
		epochs=num_epochs,
	)
	val_mse, val_mae = model.evaluate(val_data, val_targets)
	all_scores.append(val_mae) # ëª¨ë¸ í‰ê°€ í›„ í•´ë‹¹ í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ë¥¼ ì €ì¥
```

```python
all_scores
[1.8516002893447876, 2.4282381534576416, 2.4706802368164062, 2.490154266357422]

np.mean(all_scores)
2.3101682364940643
```

ê²€ì¦ ì„¸íŠ¸ê°€ ë‹¤ë¥´ë¯€ë¡œ ê²€ì¦ ì ìˆ˜ ê°„ì˜ ë³€í™”ê°€ í° ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤.


ì´ë²ˆì—ëŠ” ì—í¬í¬ë¥¼ 200ìœ¼ë¡œ ëŠ˜ë¦° í›„ ëª¨ë¸ í•™ìŠµ íˆìŠ¤í† ë¦¬ë¥¼ ì €ì¥í•˜ì—¬ ì‹œê°í™”ë¥¼ ì§„í–‰í•´ë³´ê² ë‹¤.


```python
num_epochs = 200
all_mse_histories = []

for i in range(K):
	print(f"preprocessing {i}'th fold")
	val_data = train_data[i*num_val_samples: (i+1)*num_val_samples]
  val_targets = train_targets[i*num_val_samples: (i+1)*num_val_samples]
  partial_train_data = np.concatenate([
      train_data[:i*num_val_samples],
      train_data[(i+1)*num_val_samples:]
  ])
  partial_train_targets = np.concatenate([
      train_targets[:i*num_val_samples],
      train_targets[(i+1)*num_val_samples:]
  ])
  model = build_model()

  history = model.fit(
      partial_train_data,
      partial_train_targets,
      epochs=num_epochs,
      batch_size=16,
      validation_data=(val_data, val_targets),
      verbose = 1
  )
  mae_history = history.history['val_mae']
  all_mae_histories.append(mae_history)

average_mae_history = [
    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)
]
```

```python
import matplotlib.pyplot as plt

plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)
plt.xlabel('Epochs')
plt.ylabel('Average MAE')
plt.show()
```

![](/markdown-memo/legacy/images/2f724ee4-e089-4ddc-ace9-6026574fc070.png)

í•´ë‹¹ ê·¸ë˜í”„ëŠ” ì ˆëŒ€ í‰ê·  ì˜¤ì°¨ ìµœëŒ€ì™€ ìµœì†Œ ê°’ ì‚¬ì´ê°€ ë„ˆë¬´ í¬ê¸° ë•Œë¬¸ì— ë³´ê¸°ê°€ ì–´ë µë‹¤.

10 ì—í¬í¬ ì´í›„ì˜ ì ˆëŒ€ í‰ê·  ì˜¤ì°¨ ê°œìš”ë¥¼ ê·¸ë˜í”„ë¡œ ê·¸ë ¤ë³´ì

```python
truncated = average_mae_history[10:]
plt.plot(range(1, len(truncated) + 1), truncated)
plt.xlabel('Epochs')
plt.ylabel('Average MAE')
plt.show()
```

![](/markdown-memo/legacy/images/a86568c7-c386-4c0a-884b-808385e19c61.png)


í•´ë‹¹ ê·¸ë˜í”„ë¥¼ í†µí•˜ì—¬ 120~140 ë²ˆì§¸ ì—í¬í¬ ì¦ˆìŒì— ê²€ì¦ maeê°€ ì¤„ì–´ë“œëŠ” ê²ƒì„ ë©ˆì¶˜ ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤.

ê·¸ ì´í›„ë¡œ ê³¼ëŒ€ì í•©ì´ ì¼ì–´ë‚œë‹¤ê³  íŒë‹¨ë¨ìœ¼ë¡œ 130ì—í¬í¬ê¹Œì§€ë§Œ í•™ìŠµì„ ì§„í–‰í•˜ì—¬ ë³´ê² ë‹¤.

```python
model = build_model()
model.fit(train_data, train_targets, epochs=130, batch_size=16)
test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)

Epoch 1/130
26/26 [==============================] - 1s 3ms/step - loss: 481.0276 - mae: 19.9033
Epoch 2/130
26/26 [==============================] - 0s 3ms/step - loss: 288.4814 - mae: 14.6302
Epoch 3/130
26/26 [==============================] - 0s 3ms/step - loss: 131.9535 - mae: 8.9162
.
.
.
Epoch 130/130
26/26 [==============================] - 0s 3ms/step - loss: 4.4006 - mae: 1.4911

4/4 [==============================] - 0s 3ms/step - loss: 15.3248 - mae: 2.6521
```

```python
test_mae_score
2.652055263519287
```

í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€ ê²°ê³¼ mae ì†ì‹¤ ì§€í‘œë¡œ 2.6ì´ ë‚˜ì˜¤ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ëŒ€ëµ íƒ€ê²Ÿ ê°€ê²©ê³¼ 2600ë‹¬ëŸ¬ ì •ë„ ì°¨ì´ê°€ ë‚œë‹¤ëŠ” ëœ»ì´ë‹¤.


í•´ë‹¹ ëª¨ë¸ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ë‹¤ë¥¸ ì‹¤ìŠµì—ì„œë„ ì‚¬ìš©í–ˆë˜ ëª¨ë¸ì˜ ì¸µ ë° ìœ ë‹› ê°œìˆ˜ë¥¼ ë³€ê²½í•˜ì—¬ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸ í•˜ì—¬ ë³¼ ìˆ˜ ìˆë‹¤.


# ë§¤ë“­ì§“ê¸°

- í›ˆë ¨ ë°ì´í„°ì˜ ìˆ˜ê°€ ì ë‹¤ë©´ ì¤‘ê°„ ì¸µì˜ ìˆ˜ë¥¼ ì¤„ì—¬ ê³¼ëŒ€ì í•©ì„ í”¼í•˜ë„ë¡ í•  ìˆ˜ ìˆë‹¤.
- í›ˆë ¨ ë°ì´í„°ì˜ ìˆ˜ê°€ ì ì–´ ëª¨ë¸ì„ ê²€ì¦í•˜ê¸° ì–´ë ¤ìš¸ ë–„ëŠ” K-Fold Validation ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ ìµœëŒ€í•œ ê³µì •í•˜ê²Œ í‰ê°€ í•  ìˆ˜ ìˆë‹¤.
- ê°ê¸° ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ì„ ê°€ì§„ ë°ì´í„°ì˜ ê²½ìš° ê° íŠ¹ì„±ë§ˆë‹¤ í‘œì¤€í™”ë¥¼ ì ìš©í•˜ì—¬ ì‹ ê²½ë§ì´ í•™ìŠµì„ ì˜¬ë°”ë¥´ê²Œ í•˜ë„ë¡ ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë‹¤.
- íšŒê·€ ë¬¸ì œì˜ ê²½ìš° ì‹ ê²½ë§ì˜ ë§ˆì§€ë§‰ ì¸µì—ëŠ” ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì ìš©í•œë‹¤.
- íšŒê·€ ë¬¸ì œì—ì„œ ì‚¬ìš©ë˜ëŠ” ì†ì‹¤ í•¨ìˆ˜ëŠ” ë¶„ë¥˜ ë¬¸ì œì™€ëŠ” ë‹¤ë¥´ê²Œ í‰ê·  ì œê³± ì˜¤ì°¨(mse)ë¥¼ ì‚¬ìš©í•œë‹¤.
