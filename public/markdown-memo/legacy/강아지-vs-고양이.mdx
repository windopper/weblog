---
title: "강아지 vs 고양이"
description: ""
tags: ["Tensorflow","Tutorial","ML"]
date: "2023-01-20"
thumbnail: ""
---

해당 실습에서는 합성곱 신경망을 이용하여 강아지와 고양이 사진을 입력으로 받았을때 이를 잘 분류할 수 있도록 판단하는 인공지능 모델을 만들 것입니다.


해당 실습에서는 신경망을 학습시키는 총 3가지 방식을 사용할 것입니다.

- 처음부터 모델 훈련 시키기
- 사전 훈련된 모델을 사용하여 특성 추출하기
- 사전 훈련된 모델을 세밀하게 튜닝시키기

이 뿐만 아니라 모델을 훈련시키는 과정에서 발생하는 과대적합을 막기 위한 방법인 데이터 증식 (data augmention) 기법을 적용시켜 볼 것입니다.


# 데이터 모으기

[강아지 vs 고양이 데이터셋](https://www.kaggle.com/c/dogs-vs-cats/data)

해당 링크의 데이터 셋을 사용하였습니다.

해당 데이터 셋은 25000개의 강아지와 고양이 데이터 셋을 가지고 있으며 각 클래스 마다 12500개를 가지고 있습니다.

이번 실습에서는 훈련 데이터로 이 중 2000개 만을 사용하여 적은 수의 데이터 셋으로 좋은 모델을 만드는 법에 대해서 알아볼 것입니다.

## 코랩에서 캐글 데이터 셋 내려받기


데이터를 훈련, 테스트, 검증 데이터로 분류하였습니다.

```python
import os, shutil, pathlib

original_dir = pathlib.Path('train')
new_base_dir = pathlib.Path('cats_vs_dogs_small')
def make_subset(subset_name, start_index, end_index):
  for category in ('cat', 'dog'):
    dir = new_base_dir / subset_name / category
    os.makedirs(dir)
    fnames = [f"{category}.{i}.jpg" for i in range(start_index, end_index)]
    for fname in fnames:
      shutil.copyfile(src=original_dir / fname, dst=dir / fname)

make_subset('train', start_index=0, end_index=1000)
make_subset('validation', start_index=1000, end_index=1500)
make_subset('test', start_index=1500, end_index=2500)
```


# 데이터 전처리하기

해당 데이터들은 신경망에 주입하기 전 부동 소수점 텐서의 형태로 적절하게 변환되어 있어야 합니다.

데이터가 jpeg 형태의 파일로 구성되어 있으므로 신경망에 주입하려면 대략 다음과 같은 과정을 따라야 할 것입니다.


1. 사진 파일을 읽습니다
1. jpeg 콘텐츠를 rgb 픽셀 값으로 인코딩합니다.
1. 부동 소수점 텐서의 형태로 변환합니다.
1. 동일한 크기의 이미지로 변환합니다 ( 여기서는 180X180의 크기로 전처리 할 것입니다. )
1. 배치로 묶습니다.

해당 작업을 노가다로 할 수도 있겠지만 다행스럽게도 케라스에서는 `image_dataset_from_directory()` 메서드가 있습니다.

해당 메서드는 tf.data.Dataset 객체를 반환하며 모델을 훈련하는데 있어서 효율적인 입력 파이프라인을 제공합니다.

```python
from tensorflow.keras.utils import image_dataset_from_directory

train_dataset = image_dataset_from_directory(
    new_base_dir / 'train',
    image_size=(180, 180),
    batch_size=32
)

validation_dataset = image_dataset_from_directory(
    new_base_dir / 'validation',
    image_size=(180, 180),
    batch_size=32
)

test_dataset = image_dataset_from_directory(
    new_base_dir / 'test',
    image_size=(180, 180),
    batch_size=32
)


Found 2000 files belonging to 2 classes.
Found 1000 files belonging to 2 classes.
Found 2000 files belonging to 2 classes.
```


훈련 데이터셋의 배치크기를 한번 살펴봅시다.

```python
for data_batch, labels_batch in train_dataset:
  print(data_batch.shape)
  print(labels_batch.shape)
  break

(32, 180, 180, 3)
(32,)
```

설정한 배치와 이미지 크기가 잘 적용된 것을 볼 수 있습니다.


이제 아래의 세가지 방법에서 전처리한 데이터를 사용할 수 있습니다!


<!-- Unknown block type: child_page -->

<!-- Unknown block type: child_page -->

<!-- Unknown block type: child_page -->


# 매듭짓기

- 적은 수의 데이터 셋( 25000개의 데이터 중 2000개 만을 사용)을 사용하여 좋은 모델을 만들어 내는 방법에 대해 알아 보았습니다.
- 특성 추출 방식과 이를 더욱 보완할 수 있는 미세 조정 방식을 사용하여 적은 데이터 셋 만으로도 탁월한 성능을 가진 모델을 만들 수 있었습니다.
- 컨브넷은 컴퓨터 비전 작업에 가장 뛰어난 모델로. 아주 작은 데이터 셋에서도 처음부터 훈련해서 괜찮은 성능을 낼 수 있었습니다.
- 작은 데이터 셋에서는 과대적합이 가장 큰 문제로 이를 해결하기 위해서 데이터 증식이라는 기법을 사용했습니다. 이는 이미지 데이터를 다룰 때 과대적합을 막을 수 있는 가장 강력한 방법이라고 알려져 있습니다.

