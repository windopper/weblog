---
title: "잔차 연결 (Residual connection)"
description: ""
tags: ["Tutorial","Knowledge","ML"]
date: "2023-01-20"
thumbnail: "/markdown-memo/legacy/images/6405651c-8ec7-49e6-b336-f74b7e6853de.png"
---


잔차 연결은 하위 층의 출력 텐서를 상위 층의 출력 텐서에 더하여 아래층의 표현이 네트워크 위쪽으로 흘러 갈 수 있도록 하는 기법이다.

![](/markdown-memo/legacy/images/6405651c-8ec7-49e6-b336-f74b7e6853de.png)

어떤 함수 $H(x)$가 있다고 하자

위 그림은 입력 $x$와 $x$에 대한 어떤 함수 $F(x)$의 값을 더한 함수 $H(x)$의 구조를 보여준다.

$H(x)=x+F(x)$

딥러닝 과정에서 레이어가 깊어지면 **Vanishing Gradient 같은 문제가 발생**하여 input층에 가까운 은닉층이 희미해지기 떄문에 이를 해결하기 위하여 하위 층의 텐서를 그대로 상위 층에 더하는 기법이다.

해당 기법을 통하여 input층을 그대로 가져옴으로서 잔여효과인 $F(x)$만 학습을 하면 된다. 따라서 **전체를 학습시키는 것보다 학습이 오히려 쉬워지고**, 수렴을 더 잘하게 된다.


잔차 연결 pseudo 코드는 다음과 같다

```python
x = ... 
residual = x
x = block(x)
x = add([x, residual])
```


